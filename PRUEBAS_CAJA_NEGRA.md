# üß™ Gu√≠a Completa de Pruebas de Caja Negra - Sistema RAG Aconex

## üìë Tabla de Contenidos

1. [¬øQu√© son las Pruebas de Caja Negra?](#qu√©-son-las-pruebas-de-caja-negra)
2. [Pruebas Implementadas](#pruebas-implementadas)
3. [C√≥mo Ejecutar las Pruebas](#c√≥mo-ejecutar-las-pruebas)
4. [Resultados Esperados](#resultados-esperados)
5. [Casos de Prueba Detallados](#casos-de-prueba-detallados)
6. [Troubleshooting](#troubleshooting)

---

## üéØ ¬øQu√© son las Pruebas de Caja Negra?

Las **pruebas de caja negra** (black box testing) validan el comportamiento del sistema desde el punto de vista del usuario, **sin conocer la implementaci√≥n interna**:

### Caracter√≠sticas:

‚úÖ **Se enfoca en**: Entradas ‚Üí Salidas  
‚úÖ **Valida**: Comportamiento esperado seg√∫n especificaciones  
‚úÖ **No analiza**: C√≥digo interno, algoritmos, estructuras de datos  
‚úÖ **Simula**: Uso real del sistema por parte de usuarios

### Diferencia con Caja Blanca:

| Aspecto | Caja Negra ‚úÖ | Caja Blanca ‚ùå |
|---------|---------------|----------------|
| **Conocimiento** | Solo interfaz p√∫blica | Implementaci√≥n interna |
| **Enfoque** | Funcionalidad completa | L√≥gica y rutas de c√≥digo |
| **Validaci√≥n** | Input ‚Üí Output | Cobertura de l√≠neas |
| **Mantenimiento** | Independiente de cambios internos | Requiere actualizaci√≥n con cambios |

---

## üìä Pruebas Implementadas

El proyecto cuenta con **22 pruebas de caja negra** distribuidas en 4 escenarios:

### 1Ô∏è‚É£ **Chat Conversacional RAG** (7 tests)

| Test | Archivo | Tipo | Descripci√≥n |
|------|---------|------|-------------|
| `test_chat_with_document_context` | `test_chat.py` | ‚úÖ Positivo | Chat con documentos relevantes |
| `test_chat_without_relevant_documents` | `test_chat.py` | ‚ö†Ô∏è Negativo | Chat sin contexto disponible |
| `test_save_chat_history` | `test_chat.py` | ‚úÖ Positivo | Guardar historial de conversaci√≥n |
| `test_get_chat_history` | `test_chat.py` | ‚úÖ Positivo | Recuperar historial de usuario |
| `test_chat_with_empty_question` | `test_chat.py` | ‚ö†Ô∏è Negativo | Pregunta vac√≠a |
| `test_save_chat_history_database_error` | `test_chat.py` | ‚ö†Ô∏è Negativo | Error de BD al guardar |
| `test_get_chat_history_no_results` | `test_chat.py` | ‚ö†Ô∏è Negativo | Usuario sin historial |

### 2Ô∏è‚É£ **B√∫squeda Sem√°ntica** (4 tests)

| Test | Archivo | Tipo | Descripci√≥n |
|------|---------|------|-------------|
| `test_semantic_search_basic` | `test_search.py` | ‚úÖ Positivo | B√∫squeda con resultados |
| `test_semantic_search_with_project_filter` | `test_search.py` | ‚úÖ Positivo | Filtrado por proyecto |
| `test_semantic_search_empty_query` | `test_search.py` | ‚ö†Ô∏è Negativo | Query vac√≠a |
| `test_semantic_search_invalid_project_id` | `test_search.py` | ‚ö†Ô∏è Negativo | Proyecto inexistente |

### 3Ô∏è‚É£ **Upload de Documentos** (4 tests)

| Test | Archivo | Tipo | Descripci√≥n |
|------|---------|------|-------------|
| `test_extract_text_from_txt` | `test_upload.py` | ‚úÖ Positivo | Extracci√≥n de texto plano |
| `test_generate_document_id_unique` | `test_upload.py` | ‚úÖ Positivo | Generaci√≥n de IDs √∫nicos |
| `test_extract_text_file_not_found` | `test_upload.py` | ‚ö†Ô∏è Negativo | Archivo inexistente |
| `test_extract_text_invalid_encoding` | `test_upload.py` | ‚ö†Ô∏è Negativo | Encoding corrupto |

### 4Ô∏è‚É£ **Ingesta de Datos** (4 tests)

| Test | Archivo | Tipo | Descripci√≥n |
|------|---------|------|-------------|
| `test_normalize_doc_complete` | `test_ingest.py` | ‚úÖ Positivo | Normalizaci√≥n completa |
| `test_iter_docs_from_file_json_and_ndjson` | `test_ingest.py` | ‚úÖ Positivo | Lectura JSON/NDJSON |
| `test_normalize_doc_missing_fields` | `test_ingest.py` | ‚ö†Ô∏è Negativo | Campos faltantes |
| `test_iter_docs_invalid_json` | `test_ingest.py` | ‚ö†Ô∏è Negativo | JSON malformado |

### 5Ô∏è‚É£ **Utilidades Core** (3 tests)

| Test | Archivo | Tipo | Descripci√≥n |
|------|---------|------|-------------|
| `test_simple_chunk_with_overlap` | `test_utils.py` | ‚úÖ Positivo | Chunking con overlap |
| `test_get_db_connection_success` | `test_utils.py` | ‚úÖ Positivo | Conexi√≥n a BD exitosa |
| `test_simple_chunk_invalid_parameters` | `test_utils.py` | ‚ö†Ô∏è Negativo | Par√°metros inv√°lidos |

---

## üöÄ C√≥mo Ejecutar las Pruebas

### **Prerequisitos**

```powershell
# 1. Activar entorno virtual
& .\.venv311\Scripts\Activate.ps1

# 2. Verificar instalaci√≥n de dependencias
pip install -r requirements-test.txt
```

### **Opciones de Ejecuci√≥n**

#### ‚úÖ **Ejecutar TODAS las pruebas**

```powershell
# Opci√≥n 1: Usar script helper
.\run_tests.ps1 all

# Opci√≥n 2: Comando directo
pytest tests/ -v
```

**Salida esperada:**
```
tests/test_chat.py::test_chat_with_document_context PASSED          [ 14%]
tests/test_chat.py::test_chat_without_relevant_documents PASSED     [ 28%]
tests/test_search.py::test_semantic_search_basic PASSED             [ 42%]
...
======================== 22 passed in 5.23s ========================
```

---

#### üìä **Ejecutar con reporte de cobertura**

```powershell
# Generar reporte HTML
.\run_tests.ps1 cov

# O manualmente:
pytest tests/ --cov=app --cov-report=html --cov-report=term-missing -v
```

**Ver reporte:**
```powershell
# Abrir en navegador
Start-Process htmlcov\index.html
```

---

#### üéØ **Ejecutar por categor√≠a**

```powershell
# Solo tests de Chat
pytest tests/test_chat.py -v

# Solo tests de B√∫squeda
pytest tests/test_search.py -v

# Solo tests de Upload
pytest tests/test_upload.py -v

# Solo tests de Ingesta
pytest tests/test_ingest.py -v

# Solo tests de Utilidades
pytest tests/test_utils.py -v
```

-
#### ‚ö° **Ejecuci√≥n r√°pida (paralelo)**

```powershell
# Usar todos los cores disponibles
.\run_tests.ps1 fast

# O manualmente:
pytest tests/ -n auto -v
```

---

#### üîç **Ejecutar solo tests que fallaron**

```powershell
# Re-ejecutar √∫ltimos tests fallidos
.\run_tests.ps1 failing

# O manualmente:
pytest tests/ --lf -v
```

---

#### üìù **Modo verbose con salida detallada**

```powershell
# M√°ximo detalle
pytest tests/ -vv -s

# Con traceback completo
pytest tests/ -v --tb=long
```

---

## üìà Resultados Esperados

### ‚úÖ **Ejecuci√≥n Exitosa**

```
============================== test session starts ==============================
platform win32 -- Python 3.11.0, pytest-9.0.1

collected 22 items

tests/test_chat.py::test_chat_with_document_context PASSED          [  4%]
tests/test_chat.py::test_chat_without_relevant_documents PASSED     [  9%]
tests/test_chat.py::test_save_chat_history PASSED                   [ 13%]
tests/test_chat.py::test_get_chat_history PASSED                    [ 18%]
tests/test_chat.py::test_chat_with_empty_question PASSED            [ 22%]
tests/test_chat.py::test_save_chat_history_database_error PASSED    [ 27%]
tests/test_chat.py::test_get_chat_history_no_results PASSED         [ 31%]

tests/test_search.py::test_semantic_search_basic PASSED             [ 36%]
tests/test_search.py::test_semantic_search_with_project_filter PASSED [ 40%]
tests/test_search.py::test_semantic_search_empty_query PASSED       [ 45%]
tests/test_search.py::test_semantic_search_invalid_project_id PASSED [ 50%]

tests/test_upload.py::test_extract_text_from_txt PASSED             [ 54%]
tests/test_upload.py::test_generate_document_id_unique PASSED       [ 59%]
tests/test_upload.py::test_extract_text_file_not_found PASSED       [ 63%]
tests/test_upload.py::test_extract_text_invalid_encoding PASSED     [ 68%]

tests/test_ingest.py::test_normalize_doc_complete PASSED            [ 72%]
tests/test_ingest.py::test_iter_docs_from_file_json_and_ndjson PASSED [ 77%]
tests/test_ingest.py::test_normalize_doc_missing_fields PASSED      [ 81%]
tests/test_ingest.py::test_iter_docs_invalid_json PASSED            [ 86%]

tests/test_utils.py::test_simple_chunk_with_overlap PASSED          [ 90%]
tests/test_utils.py::test_get_db_connection_success PASSED          [ 95%]
tests/test_utils.py::test_simple_chunk_invalid_parameters PASSED    [100%]

======================== 22 passed in 5.23s ========================
```

### üìä **Resumen de Cobertura**

```
---------- coverage: platform win32, python 3.11.0 -----------
Name                    Stmts   Miss  Cover   Missing
-----------------------------------------------------
app/__init__.py            12      0   100%
app/api.py                145     18    88%   45-52, 89-95
app/analytics.py           67      8    88%   34-38, 67-70
app/ingest.py              98      12   88%   67-72, 145-150
app/search_core.py        123     15   88%   89-95, 201-208
app/upload.py              87      11   87%   56-62, 134-139
app/utils.py               45      3    93%   78-80
-----------------------------------------------------
TOTAL                     577     67    88%
```

---

## üìã Casos de Prueba Detallados

### üîµ Escenario 1: Chat Conversacional RAG

#### **Test 1.1: Chat con Documentos Relevantes** ‚úÖ

**Archivo**: `tests/test_chat.py::test_chat_with_document_context`

**Prop√≥sito**: Validar el flujo completo de RAG (Retrieval-Augmented Generation)

**Entrada (Input)**:
```python
request = ChatRequest(
    question="¬øQu√© incluye el plan maestro de arquitectura?",
    max_context_docs=5,
    session_id="test-session-001"
)
```

**Salida Esperada (Output)**:
```python
response = ChatResponse(
    question="¬øQu√© incluye el plan maestro de arquitectura?",
    answer="Bas√°ndome en la documentaci√≥n t√©cnica... [respuesta generada]",
    sources=[
        {"id": "DOC-ARQ-001", "title": "Plan Maestro", "score": 0.87}
    ],
    context_used="[snippets de documentos relevantes]",
    session_id="test-session-001"
)
```

**Validaciones de Caja Negra**:
- ‚úÖ Respuesta contiene informaci√≥n del contexto
- ‚úÖ Lista de sources no est√° vac√≠a
- ‚úÖ Context_used tiene contenido sustancial (> 100 caracteres)
- ‚úÖ Session_id se preserva o genera

**Ejecutar**:
```powershell
pytest tests/test_chat.py::test_chat_with_document_context -v
```

---

#### **Test 1.2: Chat sin Documentos Relevantes** ‚ö†Ô∏è

**Archivo**: `tests/test_chat.py::test_chat_without_relevant_documents`

**Prop√≥sito**: Validar que el sistema maneja apropiadamente cuando NO hay contexto

**Entrada (Input)**:
```python
request = ChatRequest(
    question="¬øCu√°l es la receta del pastel de chocolate?",  # Fuera de contexto
    max_context_docs=5
)
```

**Salida Esperada (Output)**:
```python
response = ChatResponse(
    answer="No encuentro informaci√≥n relevante en los documentos disponibles.",
    sources=[],  # Lista vac√≠a
    context_used=""  # String vac√≠o
)
```

**Validaciones de Caja Negra**:
- ‚úÖ Sistema no crashea
- ‚úÖ Respuesta indica "no encuentro informaci√≥n"
- ‚úÖ Lista de sources est√° vac√≠a
- ‚úÖ No genera "alucinaciones" sin contexto

**Ejecutar**:
```powershell
pytest tests/test_chat.py::test_chat_without_relevant_documents -v
```

---

### üîµ Escenario 2: B√∫squeda Sem√°ntica

#### **Test 2.1: B√∫squeda B√°sica** ‚úÖ

**Archivo**: `tests/test_search.py::test_semantic_search_basic`

**Prop√≥sito**: Validar b√∫squeda sem√°ntica con ranking h√≠brido

**Entrada (Input)**:
```python
query = "construcci√≥n sismo resistente"
project_id = None
top_k = 10
```

**Salida Esperada (Output)**:
```python
results = [
    {
        "document_id": "DOC-001",
        "title": "Manual de Construcci√≥n S√≠smica",
        "snippet": "Normas NSR-10...",
        "score": 0.78  # Entre 0 y 1
    },
    ...
]
```

**Validaciones de Caja Negra**:
- ‚úÖ Retorna lista de documentos
- ‚úÖ Cada documento tiene score entre 0 y 1
- ‚úÖ Documentos ordenados por relevancia (score descendente)
- ‚úÖ No m√°s de `top_k` resultados

**Ejecutar**:
```powershell
pytest tests/test_search.py::test_semantic_search_basic -v
```

---

#### **Test 2.2: B√∫squeda con Filtro de Proyecto** ‚úÖ

**Archivo**: `tests/test_search.py::test_semantic_search_with_project_filter`

**Prop√≥sito**: Validar aislamiento multi-tenant (multi-tenancy)

**Entrada (Input)**:
```python
query = "arquitectura educativa"
project_id = "PROYECTO-EDUCATIVO"  # Filtro espec√≠fico
top_k = 20
```

**Salida Esperada (Output)**:
```python
results = [
    {"document_id": "EDU-001", "project_id": "PROYECTO-EDUCATIVO", ...},
    {"document_id": "EDU-002", "project_id": "PROYECTO-EDUCATIVO", ...},
    # TODOS los resultados del mismo proyecto
]
```

**Validaciones de Caja Negra**:
- ‚úÖ Todos los resultados pertenecen al proyecto especificado
- ‚úÖ No se mezclan documentos de otros proyectos
- ‚úÖ Aislamiento de datos garantizado

**Ejecutar**:
```powershell
pytest tests/test_search.py::test_semantic_search_with_project_filter -v
```

---

### üîµ Escenario 3: Upload de Documentos

#### **Test 3.1: Extracci√≥n de Texto TXT** ‚úÖ

**Archivo**: `tests/test_upload.py::test_extract_text_from_txt`

**Prop√≥sito**: Validar extracci√≥n b√°sica de texto plano

**Entrada (Input)**:
```python
archivo_txt = "documento.txt"
contenido = """Manual de Seguridad en Construcci√≥n
Procedimientos EPP..."""
```

**Salida Esperada (Output)**:
```python
texto_extraido = "Manual de Seguridad en Construcci√≥n\nProcedimientos EPP..."
```

**Validaciones de Caja Negra**:
- ‚úÖ Texto extra√≠do contiene palabras clave del archivo
- ‚úÖ Longitud del texto es sustancial (> 50 caracteres)
- ‚úÖ Encoding UTF-8 preservado (caracteres especiales)

**Ejecutar**:
```powershell
pytest tests/test_upload.py::test_extract_text_from_txt -v
```

---

#### **Test 3.2: Generaci√≥n de IDs √önicos** ‚úÖ

**Archivo**: `tests/test_upload.py::test_generate_document_id_unique`

**Prop√≥sito**: Validar generaci√≥n de identificadores MD5

**Entrada (Input)**:
```python
filename = "manual.txt"
content = "Contenido del documento"
```

**Salida Esperada (Output)**:
```python
document_id = "a1b2c3d4e5f6789012345678901234ab"  # Hash MD5 (32 chars)
```

**Validaciones de Caja Negra**:
- ‚úÖ ID tiene exactamente 32 caracteres hexadecimales
- ‚úÖ Solo contiene [0-9a-f]
- ‚úÖ Cambios en filename/content generan IDs diferentes
- ‚úÖ IDs son reproducibles con mismos inputs

**Ejecutar**:
```powershell
pytest tests/test_upload.py::test_generate_document_id_unique -v
```

---

### üîµ Escenario 4: Ingesta de Datos

#### **Test 4.1: Normalizaci√≥n Completa** ‚úÖ

**Archivo**: `tests/test_ingest.py::test_normalize_doc_complete`

**Prop√≥sito**: Validar transformaci√≥n de documentos Aconex

**Entrada (Input)**:
```python
documento_aconex = {
    "DocumentId": "200076-CCC02-PL-AR-000400",
    "project_id": "PROJ-TEST-001",
    "metadata": {
        "Title": "Plan Maestro",
        "Number": "200076-CCC02-PL-AR-000400",
        "Category": "Arquitectura",
        "DateModified": "2024-01-15T10:30:00Z"
    },
    "full_text": "Contenido t√©cnico..."
}
```

**Salida Esperada (Output)**:
```python
documento_normalizado = {
    "document_id": "200076-CCC02-PL-AR-000400",
    "project_id": "PROJ-TEST-001",
    "title": "Plan Maestro",
    "category": "Arquitectura",
    "body_text": "Plan Maestro\n\nContenido t√©cnico...",
    "date_modified": datetime(2024, 1, 15, 10, 30, 0)
}
```

**Validaciones de Caja Negra**:
- ‚úÖ Todos los campos se extraen correctamente
- ‚úÖ `date_modified` es tipo `datetime`, no string
- ‚úÖ `body_text` concatena t√≠tulo + contenido
- ‚úÖ Prioridad de `project_id` correcta

**Ejecutar**:
```powershell
pytest tests/test_ingest.py::test_normalize_doc_complete -v
```

---

#### **Test 4.2: Lectura JSON y NDJSON** ‚úÖ

**Archivo**: `tests/test_ingest.py::test_iter_docs_from_file_json_and_ndjson`

**Prop√≥sito**: Validar soporte para ambos formatos

**Entrada 1 (JSON Array)**:
```json
[
    {"DocumentId": "001", "metadata": {"Title": "Doc 1"}},
    {"DocumentId": "002", "metadata": {"Title": "Doc 2"}}
]
```

**Entrada 2 (NDJSON)**:
```json
{"DocumentId": "003", "metadata": {"Title": "Doc 3"}}
{"DocumentId": "004", "metadata": {"Title": "Doc 4"}}
```

**Salida Esperada**:
```python
# Ambos formatos retornan lista de documentos
docs = [
    {"DocumentId": "001", ...},
    {"DocumentId": "002", ...}
]
```

**Validaciones de Caja Negra**:
- ‚úÖ Detecci√≥n autom√°tica de formato
- ‚úÖ Parsing correcto de ambos formatos
- ‚úÖ L√≠neas vac√≠as en NDJSON se ignoran
- ‚úÖ N√∫mero correcto de documentos extra√≠dos

**Ejecutar**:
```powershell
pytest tests/test_ingest.py::test_iter_docs_from_file_json_and_ndjson -v
```

---

## üîß Troubleshooting

### ‚ùå **Error: ModuleNotFoundError: No module named 'app'**

**Causa**: pytest no encuentra el m√≥dulo `app`

**Soluci√≥n**:
```powershell
# Verificar que est√°s en backend-acorag/
cd backend-acorag

# Verificar estructura
ls app/

# Ejecutar desde directorio correcto
pytest tests/ -v
```

---

### ‚ùå **Error: fixture 'mock_model_loader' not found**

**Causa**: `conftest.py` no se est√° cargando

**Soluci√≥n**:
```powershell
# Verificar que conftest.py existe
ls tests/conftest.py

# Asegurarse de que __init__.py existe
ls tests/__init__.py

# Re-ejecutar
pytest tests/ -v
```

---

### ‚ùå **Error: FAILED tests/test_chat.py - ImportError: cannot import name 'ChatRequest'**

**Causa**: M√≥dulos de `app/` no disponibles

**Soluci√≥n**:
```powershell
# Verificar que app/ tiene los m√≥dulos
ls app/api.py
ls app/analytics.py

# Verificar Python Path
python -c "import sys; print('\n'.join(sys.path))"

# Instalar en modo editable
pip install -e .
```

---

### ‚ö†Ô∏è **Warning: No coverage data collected**

**Causa**: pytest-cov no instalado o mal configurado

**Soluci√≥n**:
```powershell
# Instalar pytest-cov
pip install pytest-cov

# Verificar instalaci√≥n
pytest --version

# Ejecutar con cobertura
pytest tests/ --cov=app --cov-report=term
```

---

### üêå **Tests muy lentos**

**Causa**: Ejecuci√≥n secuencial

**Soluci√≥n**:
```powershell
# Instalar pytest-xdist
pip install pytest-xdist

# Ejecutar en paralelo (todos los cores)
pytest tests/ -n auto

# O especificar n√∫mero de workers
pytest tests/ -n 4
```

---


