# âš¡ GuÃ­a RÃ¡pida - Pruebas SemÃ¡nticas RAG

## ğŸ¯ Â¿QuÃ© son estas pruebas?

EvalÃºan la **calidad de las respuestas** del sistema RAG usando 3 mÃ©tricas:

1. **BERT Score** â†’ Â¿El significado es correcto? ğŸ§ 
2. **ROUGE** â†’ Â¿Usa las palabras apropiadas? ğŸ“
3. **WER** â†’ Â¿QuÃ© tan diferente es del texto original? ğŸ”

---

## ğŸš€ Inicio RÃ¡pido (2 minutos)

### 1. Instalar Dependencias

```bash
pip install bert-score rouge-score jiwer pytest
```

### 2. Ejecutar Pruebas

```bash
cd backend-acorag
pytest tests/test_semantic_evaluation.py -v
```

### 3. Ver Resultados

```bash
cat reports/evaluacion_completa.txt
```

---

## ğŸ“Š Resultados Actuales

```
âœ… BERT F1:        0.8335  (Excelente similitud semÃ¡ntica)
âœ… ROUGE-1 F1:     0.4558  (Buena cobertura lÃ©xica)
âœ… ROUGE-2 F1:     0.2022  (Bigramas consistentes)
âœ… ROUGE-L F1:     0.4097  (Buena estructura)
âš ï¸ Word Accuracy:  0.2237  (ReformulaciÃ³n - normal en RAG)

VEREDICTO: âœ… SISTEMA APROBADO
```

---

## ğŸ¯ InterpretaciÃ³n Simple

### âœ… Â¿El sistema funciona bien?

**SÃ** â†’ BERT Score = 0.83 (Excelente)

El sistema comprende las preguntas y genera respuestas correctas.

### âš ï¸ Â¿Por quÃ© Word Accuracy es baja?

**Es NORMAL** â†’ Los sistemas RAG reformulan informaciÃ³n

```
No copia literal â†’ Genera respuestas originales
BERT alto (0.83) â†’ El significado es correcto
â†’ No hay problema âœ…
```

---

## ğŸ“š DocumentaciÃ³n Completa

| Documento | DescripciÃ³n | TamaÃ±o |
|-----------|-------------|--------|
| [RESUMEN_EJECUTIVO_SEMANTICAS.md](./RESUMEN_EJECUTIVO_SEMANTICAS.md) | Dashboard con mÃ©tricas | 1 pÃ¡gina |
| [PRUEBAS_SEMANTICAS_RAG.md](./PRUEBAS_SEMANTICAS_RAG.md) | GuÃ­a completa tÃ©cnica | 50+ pÃ¡ginas |
| [tests/test_semantic_evaluation.py](./tests/test_semantic_evaluation.py) | CÃ³digo de las pruebas | Script Python |

---

## ğŸ”§ Comandos Ãštiles

### Ver solo BERT Score
```bash
pytest tests/test_semantic_evaluation.py::test_bert_score_promedio -v
```

### Ver solo ROUGE
```bash
pytest tests/test_semantic_evaluation.py::test_rouge_promedio -v
```

### Ver solo WER
```bash
pytest tests/test_semantic_evaluation.py::test_wer_promedio -v
```

### Generar reporte completo
```bash
pytest tests/test_semantic_evaluation.py::test_evaluacion_completa -v
```

---

## ğŸ“ˆ Â¿QuÃ© mÃ©trica importa mÃ¡s?

### Para usuarios tÃ©cnicos:
**BERT Score** â†’ Mide si el sistema entiende correctamente

### Para usuarios de negocio:
**ROUGE-1** â†’ Mide si usa el vocabulario tÃ©cnico apropiado

### Para desarrolladores:
**WER** â†’ Mide cuÃ¡nto reformula (alto = no copia literal)

---

## âœ… Checklist de Calidad

| MÃ©trica | Umbral | Actual | Estado |
|---------|--------|--------|--------|
| BERT F1 | > 0.75 | **0.83** | âœ… |
| ROUGE-1 | > 0.30 | **0.46** | âœ… |
| ROUGE-2 | > 0.15 | **0.20** | âœ… |
| ROUGE-L | > 0.25 | **0.41** | âœ… |

**RESULTADO: 4/4 APROBADAS** âœ…

---

## ğŸš¨ CuÃ¡ndo Preocuparse

### ğŸ”´ Alerta Roja (AcciÃ³n inmediata)
- BERT F1 < 0.70 â†’ El sistema no comprende
- ROUGE-1 < 0.25 â†’ Vocabulario incorrecto

### ğŸŸ¡ Alerta Amarilla (Revisar)
- BERT F1 < 0.75 â†’ Revisar casos problemÃ¡ticos
- ROUGE-1 < 0.30 â†’ Mejorar cobertura lÃ©xica

### ğŸŸ¢ Estado Saludable (Actual)
- BERT F1 > 0.80 â†’ âœ… Excelente comprensiÃ³n
- ROUGE-1 > 0.40 â†’ âœ… Buen vocabulario

---

## ğŸ’¡ PrÃ³ximos Pasos

1. âœ… **Sistema aprobado** - Listo para producciÃ³n
2. ğŸ“‹ Configurar monitoreo continuo
3. ğŸ“‹ Recopilar feedback de usuarios
4. ğŸ“‹ Expandir dataset de evaluaciÃ³n (20-30 casos)

---

## ğŸ”— Enlaces RÃ¡pidos

- ğŸ“Š [Ver reporte completo](./reports/evaluacion_completa.txt)
- ğŸ“ˆ [Dashboard ejecutivo](./RESUMEN_EJECUTIVO_SEMANTICAS.md)
- ğŸ“š [DocumentaciÃ³n tÃ©cnica](./PRUEBAS_SEMANTICAS_RAG.md)
- ğŸ§ª [CÃ³digo de pruebas](./tests/test_semantic_evaluation.py)

---

**Generado:** Diciembre 2024  
**Tiempo de lectura:** 2 minutos  
**Nivel:** Principiante-Intermedio
