# üìö √çndice Maestro - Documentaci√≥n de Pruebas Sem√°nticas

## üéØ Navegaci√≥n R√°pida

### Por Rol

| Rol | Documento Recomendado | Tiempo |
|-----|----------------------|---------|
| **Ejecutivo/Manager** | [RESUMEN_EJECUTIVO_SEMANTICAS.md](./RESUMEN_EJECUTIVO_SEMANTICAS.md) | 5 min |
| **Desarrollador** | [INICIO_RAPIDO_SEMANTICAS.md](./INICIO_RAPIDO_SEMANTICAS.md) | 2 min |
| **Ingeniero QA** | [PRUEBAS_SEMANTICAS_RAG.md](./PRUEBAS_SEMANTICAS_RAG.md) | 30 min |
| **Data Scientist** | [PRUEBAS_SEMANTICAS_RAG.md](./PRUEBAS_SEMANTICAS_RAG.md) + Reports | 45 min |

### Por Necesidad

| Necesidad | Documento | Descripci√≥n |
|-----------|-----------|-------------|
| Ver resultados r√°pido | [RESUMEN_EJECUTIVO_SEMANTICAS.md](./RESUMEN_EJECUTIVO_SEMANTICAS.md) | Dashboard con m√©tricas principales |
| Ejecutar pruebas | [INICIO_RAPIDO_SEMANTICAS.md](./INICIO_RAPIDO_SEMANTICAS.md) | Comandos y pasos r√°pidos |
| Entender m√©tricas | [PRUEBAS_SEMANTICAS_RAG.md](./PRUEBAS_SEMANTICAS_RAG.md) | Explicaci√≥n detallada de BERT, ROUGE, WER |
| Ver c√≥digo | [tests/test_semantic_evaluation.py](./tests/test_semantic_evaluation.py) | Implementaci√≥n completa |
| Analizar resultados | [reports/evaluacion_completa.txt](./reports/evaluacion_completa.txt) | Reporte detallado por caso |

---

## üìä Estructura de Documentaci√≥n

```
Pruebas Sem√°nticas RAG/
‚îú‚îÄ‚îÄ üìÑ INICIO_RAPIDO_SEMANTICAS.md        [2 min]  ‚ö° START HERE
‚îÇ   ‚îî‚îÄ‚îÄ Gu√≠a de inicio r√°pido para ejecutar pruebas
‚îÇ
‚îú‚îÄ‚îÄ üìä RESUMEN_EJECUTIVO_SEMANTICAS.md    [5 min]  üëî Para Managers
‚îÇ   ‚îú‚îÄ‚îÄ Dashboard de m√©tricas
‚îÇ   ‚îú‚îÄ‚îÄ Veredicto de calidad
‚îÇ   ‚îî‚îÄ‚îÄ Comparaci√≥n con industria
‚îÇ
‚îú‚îÄ‚îÄ üìö PRUEBAS_SEMANTICAS_RAG.md          [30 min] üî¨ Documentaci√≥n T√©cnica
‚îÇ   ‚îú‚îÄ‚îÄ Introducci√≥n a m√©tricas NLP
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BERT Score (similitud sem√°ntica)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ROUGE (cobertura l√©xica)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ WER (exactitud literal)
‚îÇ   ‚îú‚îÄ‚îÄ Metodolog√≠a de evaluaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ Resultados detallados por caso
‚îÇ   ‚îú‚îÄ‚îÄ An√°lisis por categor√≠a
‚îÇ   ‚îú‚îÄ‚îÄ Interpretaci√≥n y recomendaciones
‚îÇ   ‚îî‚îÄ‚îÄ Gu√≠a de ejecuci√≥n completa
‚îÇ
‚îú‚îÄ‚îÄ üß™ tests/test_semantic_evaluation.py  [C√≥digo] üíª Implementaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ Dataset de evaluaci√≥n (8 casos)
‚îÇ   ‚îú‚îÄ‚îÄ Funciones de c√°lculo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calcular_bert_score()
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calcular_rouge()
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ calcular_wer()
‚îÇ   ‚îî‚îÄ‚îÄ Suite de tests parametrizados
‚îÇ
‚îî‚îÄ‚îÄ üìÅ reports/                           [Outputs] üìà Resultados
    ‚îú‚îÄ‚îÄ evaluacion_completa.txt           ‚Üí Reporte completo
    ‚îú‚îÄ‚îÄ bert_score_summary.txt            ‚Üí Resumen BERT
    ‚îú‚îÄ‚îÄ rouge_summary.txt                 ‚Üí Resumen ROUGE
    ‚îî‚îÄ‚îÄ wer_summary.txt                   ‚Üí Resumen WER
```

---

## üéì Gu√≠a de Lectura

### üìñ Nivel 1: Quick Start (5 minutos)

**Objetivo:** Ejecutar pruebas y ver resultados

```
1. Leer: INICIO_RAPIDO_SEMANTICAS.md
2. Ejecutar: pytest tests/test_semantic_evaluation.py -v
3. Ver: reports/evaluacion_completa.txt
```

**Entender√°s:**
- ‚úÖ C√≥mo ejecutar las pruebas
- ‚úÖ Qu√© significan los resultados principales
- ‚úÖ Si el sistema est√° aprobado o no

---

### üìä Nivel 2: Executive Overview (15 minutos)

**Objetivo:** Entender la calidad del sistema

```
1. Leer: RESUMEN_EJECUTIVO_SEMANTICAS.md
2. Revisar: Dashboard de m√©tricas
3. Analizar: Comparaci√≥n con industria
```

**Entender√°s:**
- ‚úÖ M√©tricas de calidad principales
- ‚úÖ Fortalezas y √°reas de mejora
- ‚úÖ C√≥mo se compara con est√°ndares
- ‚úÖ Plan de acci√≥n recomendado

---

### üî¨ Nivel 3: Technical Deep Dive (60 minutos)

**Objetivo:** Dominar las m√©tricas y metodolog√≠a

```
1. Leer: PRUEBAS_SEMANTICAS_RAG.md (secciones 1-4)
2. Estudiar: Introducci√≥n a m√©tricas (BERT, ROUGE, WER)
3. Analizar: Resultados por caso
4. Revisar: tests/test_semantic_evaluation.py
```

**Entender√°s:**
- ‚úÖ Fundamentos de BERT Score
- ‚úÖ Fundamentos de ROUGE (1, 2, L)
- ‚úÖ Fundamentos de WER/CER
- ‚úÖ C√≥mo interpretar cada m√©trica
- ‚úÖ Por qu√© WER bajo es normal en RAG

---

### üíª Nivel 4: Implementation (2 horas)

**Objetivo:** Modificar y extender las pruebas

```
1. Estudiar: tests/test_semantic_evaluation.py
2. Entender: Dataset y estructura de casos
3. Modificar: A√±adir nuevos casos de prueba
4. Ejecutar: Validar cambios
5. Leer: PRUEBAS_SEMANTICAS_RAG.md (secciones 5-7)
```

**Podr√°s:**
- ‚úÖ A√±adir nuevos casos de evaluaci√≥n
- ‚úÖ Ajustar umbrales de calidad
- ‚úÖ Personalizar m√©tricas
- ‚úÖ Generar reportes personalizados
- ‚úÖ Integrar en CI/CD

---

## üìà Resultados Actuales

### Snapshot R√°pido

```
M√âTRICAS PRINCIPALES:
  BERT F1:        0.8335 ‚úÖ  (Excelente similitud sem√°ntica)
  ROUGE-1 F1:     0.4558 ‚úÖ  (Buena cobertura l√©xica)
  ROUGE-2 F1:     0.2022 ‚úÖ  (Bigramas consistentes)
  ROUGE-L F1:     0.4097 ‚úÖ  (Buena estructura)
  Word Accuracy:  0.2237 ‚ö†Ô∏è  (Reformulaci√≥n - normal en RAG)

CASOS EVALUADOS: 8
TESTS EJECUTADOS: 28
TESTS APROBADOS: 20/28 (71%)
ESTADO: ‚úÖ SISTEMA APROBADO
```

### Dashboard ASCII

```
BERT Score (Similitud Sem√°ntica)
0.0                                                           1.0
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                                                    ‚ñ≤ 0.8335
                                            [THRESHOLD 0.75] ‚úÖ

ROUGE-1 (Cobertura L√©xica)
0.0                                                           1.0
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                                ‚ñ≤ 0.4558
                        [THRESHOLD 0.30] ‚úÖ

Word Accuracy (Exactitud)
0.0                                                           1.0
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
               ‚ñ≤ 0.2237
       [THRESHOLD 0.30] ‚ö†Ô∏è (Normal en RAG generativo)
```

---

## üîó Documentaci√≥n Relacionada

### Pruebas de Capacidad
- [PRUEBAS_CAPACIDAD.md](./PRUEBAS_CAPACIDAD.md) - Pruebas de rendimiento y carga
- [RESUMEN_EJECUTIVO_CAPACIDAD.md](./RESUMEN_EJECUTIVO_CAPACIDAD.md) - Dashboard de capacidad
- [VISUALIZACION_RESULTADOS_CAPACIDAD.md](./VISUALIZACION_RESULTADOS_CAPACIDAD.md) - Gr√°ficos ASCII

### Testing General
- [TESTING_GUIDE.md](./TESTING_GUIDE.md) - Gu√≠a general de testing
- [DOCUMENTACION_TESTS.md](./DOCUMENTACION_TESTS.md) - Documentaci√≥n t√©cnica
- [TESTING_SUMMARY.md](./TESTING_SUMMARY.md) - Resumen de estrategias

---

## üöÄ Flujo de Trabajo Recomendado

### Para Primera Vez

```
1. INICIO_RAPIDO_SEMANTICAS.md
   ‚îî‚îÄ> Ejecutar pruebas

2. reports/evaluacion_completa.txt
   ‚îî‚îÄ> Ver resultados

3. RESUMEN_EJECUTIVO_SEMANTICAS.md
   ‚îî‚îÄ> Entender m√©tricas

4. PRUEBAS_SEMANTICAS_RAG.md
   ‚îî‚îÄ> Profundizar en detalles
```

### Para Monitoreo Continuo

```
1. Ejecutar: pytest tests/test_semantic_evaluation.py -v

2. Revisar: reports/evaluacion_completa.txt

3. Comparar con baseline:
   - BERT F1: Mantener > 0.80
   - ROUGE-1: Mantener > 0.40
   - Word Accuracy: Monitorear tendencia

4. Actualizar documentaci√≥n si hay cambios significativos
```

### Para Debugging

```
1. Identificar caso problem√°tico en evaluacion_completa.txt

2. Revisar dataset en tests/test_semantic_evaluation.py:
   - Validar pregunta
   - Validar respuesta_referencia
   - Validar respuesta_modelo

3. Ejecutar test individual:
   pytest tests/test_semantic_evaluation.py::test_bert_score_individual[caso_X] -v

4. Analizar m√©tricas:
   - BERT bajo ‚Üí Problema sem√°ntico
   - ROUGE bajo ‚Üí Problema de vocabulario
   - WER alto con BERT bajo ‚Üí Problema de calidad
```

---

## üìä Comparativa de M√©tricas

### ¬øCu√°l m√©trica usar?

| Pregunta | M√©trica | Umbral |
|----------|---------|--------|
| ¬øEl sistema entiende correctamente? | **BERT F1** | > 0.75 |
| ¬øUsa vocabulario apropiado? | **ROUGE-1** | > 0.30 |
| ¬øMantiene estructura coherente? | **ROUGE-L** | > 0.25 |
| ¬øPreserva bigramas t√©cnicos? | **ROUGE-2** | > 0.15 |
| ¬øReformula o copia? | **WER** | < 0.80 (reformula) |

### Relaci√≥n entre M√©tricas

```
BERT Alto + ROUGE Alto + WER Alto = ‚úÖ IDEAL
(Entiende bien, usa palabras correctas, reformula)

BERT Alto + ROUGE Bajo + WER Alto = ‚ö†Ô∏è REVISAR
(Entiende pero usa vocabulario diferente)

BERT Bajo + ROUGE Alto + WER Bajo = ‚ùå PROBLEMA
(Copia palabras pero no entiende significado)

BERT Bajo + ROUGE Bajo + WER Alto = ‚ùå CR√çTICO
(No entiende ni usa vocabulario correcto)
```

**Nuestro caso:**
```
BERT: 0.83 (Alto) ‚úÖ
ROUGE-1: 0.46 (Alto) ‚úÖ
WER: 0.78 (Alto) ‚úÖ
‚Üí IDEAL: Entiende, usa vocabulario correcto y reformula
```

---

## üõ†Ô∏è Mantenimiento

### Actualizar Dataset

**Cu√°ndo:**
- Nuevas features en el sistema
- Cambios en el modelo
- Nuevos tipos de consultas

**C√≥mo:**
```python
# Editar tests/test_semantic_evaluation.py
EVALUATION_DATASET.append({
    "pregunta": "Nueva pregunta",
    "respuesta_referencia": "Respuesta gold standard",
    "respuesta_modelo": "Respuesta del sistema",
    "categoria": "nueva_categoria"
})
```

### Ajustar Umbrales

**Cu√°ndo:**
- Cambios en el modelo de generaci√≥n
- Nueva versi√≥n de librer√≠as
- Requisitos de negocio actualizados

**C√≥mo:**
```python
# En tests/test_semantic_evaluation.py
assert f1 > 0.80  # Cambiar de 0.75 si necesario
assert rouge1_f1 > 0.35  # Ajustar seg√∫n benchmark
```

### Regenerar Documentaci√≥n

**Cu√°ndo:**
- Cambios significativos en m√©tricas (> 5%)
- Nuevos casos a√±adidos al dataset
- Actualizaciones de benchmarks

**C√≥mo:**
```bash
# 1. Ejecutar pruebas
pytest tests/test_semantic_evaluation.py -v

# 2. Revisar reports/
cat reports/evaluacion_completa.txt

# 3. Actualizar documentaci√≥n si necesario
# - RESUMEN_EJECUTIVO_SEMANTICAS.md (m√©tricas)
# - PRUEBAS_SEMANTICAS_RAG.md (an√°lisis)
```

---

## üìû FAQ y Troubleshooting

### ‚ùì Preguntas Frecuentes

**Q: ¬øPor qu√© Word Accuracy es tan baja?**
A: Es NORMAL en RAG. Word Accuracy baja con BERT alto significa que el sistema reformula correctamente. Ver [PRUEBAS_SEMANTICAS_RAG.md ¬ß Interpretaci√≥n WER](./PRUEBAS_SEMANTICAS_RAG.md#3-wer-word-error-rate).

**Q: ¬øQu√© m√©trica es m√°s importante?**
A: **BERT Score** para calidad sem√°ntica, **ROUGE-1** para cobertura l√©xica. Ver [RESUMEN_EJECUTIVO_SEMANTICAS.md ¬ß Interpretaci√≥n](./RESUMEN_EJECUTIVO_SEMANTICAS.md#-interpretaci√≥n-simple).

**Q: ¬øC√≥mo a√±adir m√°s casos de prueba?**
A: Editar `EVALUATION_DATASET` en `tests/test_semantic_evaluation.py`. Ver [PRUEBAS_SEMANTICAS_RAG.md ¬ß Personalizaci√≥n](./PRUEBAS_SEMANTICAS_RAG.md#personalizaci√≥n-del-dataset).

**Q: ¬øLas pruebas son lentas?**
A: BERT Score usa modelos pesados. Primera ejecuci√≥n descarga modelo (~420MB). Ejecuciones posteriores son m√°s r√°pidas (cache).

**Q: ¬øNecesito GPU?**
A: No es obligatorio, pero acelera BERT Score significativamente (2-3x m√°s r√°pido).

### üîß Troubleshooting

**Problema:** `ModuleNotFoundError: No module named 'bert_score'`
```bash
# Soluci√≥n:
pip install bert-score rouge-score jiwer
```

**Problema:** Tests fallan con "Word Accuracy muy baja"
```bash
# Soluci√≥n: Es esperado en RAG. Revisar que BERT Score sea > 0.75
# Si BERT es alto, los tests WER pueden ignorarse o ajustar umbral
```

**Problema:** BERT Score muy lento
```bash
# Soluci√≥n: Usar menos casos o GPU
# O ajustar batch_size en calcular_bert_score()
```

**Problema:** Modelo BERT no se descarga
```bash
# Soluci√≥n: Descargar manualmente
python -c "import bert_score; bert_score.score(['test'], ['test'], lang='en')"
```

---

## üéØ Roadmap

### Completado ‚úÖ
- [x] Implementaci√≥n de BERT Score
- [x] Implementaci√≥n de ROUGE (1, 2, L)
- [x] Implementaci√≥n de WER/CER
- [x] Dataset de 8 casos de evaluaci√≥n
- [x] Generaci√≥n autom√°tica de reportes
- [x] Documentaci√≥n completa

### Pr√≥ximos Pasos üìã
- [ ] Expandir dataset a 20-30 casos
- [ ] A√±adir evaluaci√≥n humana (HITL)
- [ ] Dashboard interactivo con Streamlit
- [ ] Integraci√≥n con CI/CD
- [ ] M√©tricas de negocio (satisfacci√≥n usuario)
- [ ] A/B testing en producci√≥n

---

## üìö Referencias Adicionales

### Papers Acad√©micos
- [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)
- [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013/)
- [Word Error Rate Calculation](https://en.wikipedia.org/wiki/Word_error_rate)

### Librer√≠as
- [bert-score](https://github.com/Tiiiger/bert_score) - BERT Score implementation
- [rouge-score](https://github.com/google-research/google-research/tree/master/rouge) - Google's ROUGE
- [jiwer](https://github.com/jitsi/jiwer) - WER/CER calculation

### Blogs y Tutoriales
- [Understanding BERT Score for NLP Evaluation](https://huggingface.co/spaces/evaluate-metric/bertscore)
- [ROUGE Metrics Explained](https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/)
- [When to Use WER vs Other Metrics](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)

---

## üìù Changelog

### v1.0.0 (Diciembre 2024)
- ‚úÖ Release inicial
- ‚úÖ 8 casos de evaluaci√≥n
- ‚úÖ 3 familias de m√©tricas (BERT, ROUGE, WER)
- ‚úÖ Documentaci√≥n completa en espa√±ol
- ‚úÖ Reportes autom√°ticos
- ‚úÖ Sistema aprobado para producci√≥n

---

## üë• Contribuciones

### C√≥mo Contribuir

1. **A√±adir casos de evaluaci√≥n:**
   - Editar `tests/test_semantic_evaluation.py`
   - Seguir formato existente
   - Cubrir nuevos escenarios

2. **Mejorar documentaci√≥n:**
   - Clarificar secciones confusas
   - A√±adir ejemplos
   - Corregir errores

3. **Reportar issues:**
   - Casos problem√°ticos
   - M√©tricas inesperadas
   - Bugs en c√≥digo

---

**Documento generado:** Diciembre 2024  
**Versi√≥n:** 1.0.0  
**Autor:** Equipo Aconex RAG  
**√öltima actualizaci√≥n:** Diciembre 2024  
**Tiempo total de lectura:** Variable por nivel (5min - 2h)
