# ğŸ§  Pruebas SemÃ¡nticas RAG - Sistema Aconex

> EvaluaciÃ³n de calidad de respuestas usando mÃ©tricas NLP de la industria

[![Tests](https://img.shields.io/badge/tests-20%2F28%20passed-brightgreen)]()
[![BERT](https://img.shields.io/badge/BERT%20F1-0.8335-brightgreen)]()
[![ROUGE-1](https://img.shields.io/badge/ROUGE--1-0.4558-brightgreen)]()
[![Status](https://img.shields.io/badge/status-APROBADO-success)]()

---

## ğŸ¯ Â¿QuÃ© son estas pruebas?

Las **pruebas semÃ¡nticas** evalÃºan la **calidad** de las respuestas generadas por el sistema RAG, utilizando mÃ©tricas estÃ¡ndar de NLP:

- **BERT Score** ğŸ§  â†’ Â¿El sistema comprende correctamente? (Similitud semÃ¡ntica)
- **ROUGE** ğŸ“ â†’ Â¿Usa el vocabulario apropiado? (Cobertura lÃ©xica)
- **WER** ğŸ” â†’ Â¿Reformula o copia? (Exactitud literal)

---

## âš¡ Inicio RÃ¡pido (2 minutos)

```bash
# 1. Instalar dependencias
pip install bert-score rouge-score jiwer pytest

# 2. Ejecutar pruebas
cd backend-acorag
pytest tests/test_semantic_evaluation.py -v

# 3. Ver resultados
cat reports/evaluacion_completa.txt
```

---

## ğŸ“Š Resultados Actuales

```
âœ… BERT F1:        0.8335  (Excelente similitud semÃ¡ntica)
âœ… ROUGE-1 F1:     0.4558  (Buena cobertura lÃ©xica)
âœ… ROUGE-2 F1:     0.2022  (Bigramas consistentes)
âœ… ROUGE-L F1:     0.4097  (Buena estructura)
âš ï¸ Word Accuracy:  0.2237  (ReformulaciÃ³n - normal en RAG)

VEREDICTO: âœ… SISTEMA APROBADO PARA PRODUCCIÃ“N
CALIDAD:   â­â­â­â­ (4/5 estrellas)
```

### Dashboard Visual

```
BERT Score    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–‘  83.4%
ROUGE-1       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  45.6%
ROUGE-L       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  41.0%
Word Accuracy â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  22.4%
```

---

## ğŸ“š DocumentaciÃ³n

### Por Rol

| Rol | Documento | DescripciÃ³n | Tiempo |
|-----|-----------|-------------|--------|
| ğŸ‘” **Manager** | [RESUMEN_EJECUTIVO_SEMANTICAS.md](./RESUMEN_EJECUTIVO_SEMANTICAS.md) | Dashboard ejecutivo con mÃ©tricas | 5 min |
| ğŸ’» **Developer** | [INICIO_RAPIDO_SEMANTICAS.md](./INICIO_RAPIDO_SEMANTICAS.md) | GuÃ­a rÃ¡pida de ejecuciÃ³n | 2 min |
| ğŸ”¬ **QA Engineer** | [PRUEBAS_SEMANTICAS_RAG.md](./PRUEBAS_SEMANTICAS_RAG.md) | DocumentaciÃ³n tÃ©cnica completa | 30 min |
| ğŸ“Š **Data Scientist** | [VISUALIZACION_RESULTADOS_SEMANTICAS.md](./VISUALIZACION_RESULTADOS_SEMANTICAS.md) | AnÃ¡lisis visual y grÃ¡ficos | 15 min |

### NavegaciÃ³n RÃ¡pida

```
ğŸ“ Pruebas SemÃ¡nticas/
â”œâ”€â”€ ğŸ“„ README_PRUEBAS_SEMANTICAS.md              ğŸ‘ˆ EstÃ¡s aquÃ­
â”œâ”€â”€ âš¡ INICIO_RAPIDO_SEMANTICAS.md               [2 min]  START HERE
â”œâ”€â”€ ğŸ“Š RESUMEN_EJECUTIVO_SEMANTICAS.md           [5 min]  Para managers
â”œâ”€â”€ ğŸ“š PRUEBAS_SEMANTICAS_RAG.md                 [30 min] DocumentaciÃ³n tÃ©cnica
â”œâ”€â”€ ğŸ“ˆ VISUALIZACION_RESULTADOS_SEMANTICAS.md    [15 min] GrÃ¡ficos ASCII
â”œâ”€â”€ ğŸ“‘ INDICE_MAESTRO_SEMANTICAS.md              [10 min] Ãndice completo
â”œâ”€â”€ ğŸ§ª tests/test_semantic_evaluation.py         [CÃ³digo] ImplementaciÃ³n
â””â”€â”€ ğŸ“ reports/                                  [Output] Resultados
    â”œâ”€â”€ evaluacion_completa.txt
    â”œâ”€â”€ bert_score_summary.txt
    â”œâ”€â”€ rouge_summary.txt
    â””â”€â”€ wer_summary.txt
```

---

## ğŸ§ª CÃ³mo Funcionan las Pruebas

### 1. Dataset de EvaluaciÃ³n

8 casos de prueba cubriendo diferentes categorÃ­as:

| # | CategorÃ­a | Ejemplo | BERT F1 | ROUGE-1 |
|---|-----------|---------|---------|---------|
| 1 | DefiniciÃ³n | Â¿QuÃ© es el sistema? | 0.859 | 0.533 |
| 2 | TÃ©cnica | Â¿CÃ³mo funciona bÃºsqueda? | 0.838 | 0.449 |
| 3 | Arquitectura | Â¿QuÃ© base de datos? | 0.846 | 0.370 |
| 4 | Performance | Â¿Tiempo de respuesta? | 0.785 | 0.400 |
| 5 | Procesamiento | Â¿CÃ³mo se procesan PDFs? | 0.837 | 0.400 |
| 6 | Modelo | Â¿QuÃ© modelo embeddings? | 0.860 | 0.500 |
| 7 | Capacidad | Â¿Usuarios concurrentes? | 0.817 | 0.457 |
| 8 | API | Â¿QuÃ© endpoints? | 0.827 | 0.537 |

### 2. MÃ©tricas Calculadas

```python
# BERT Score - Similitud semÃ¡ntica profunda
bert_score(referencia, modelo) â†’ F1: 0.0-1.0

# ROUGE - Coincidencia de n-gramas
rouge_1(referencia, modelo)  â†’ Unigrams
rouge_2(referencia, modelo)  â†’ Bigrams
rouge_L(referencia, modelo)  â†’ Longest Common Subsequence

# WER - Exactitud literal
wer(referencia, modelo)      â†’ Word Error Rate
word_accuracy = 1 - WER      â†’ Accuracy
```

### 3. InterpretaciÃ³n

| MÃ©trica | Valor | Umbral | Significa |
|---------|-------|--------|-----------|
| **BERT F1** | 0.83 | > 0.75 | âœ… El sistema **entiende** correctamente |
| **ROUGE-1** | 0.46 | > 0.30 | âœ… Usa **vocabulario** apropiado |
| **ROUGE-L** | 0.41 | > 0.25 | âœ… Mantiene **estructura** coherente |
| **Word Acc** | 0.22 | > 0.30 | âš ï¸ **Reformula** (normal en RAG) |

---

## âš ï¸ FAQ - Word Accuracy Baja

### Â¿Por quÃ© Word Accuracy es solo 0.22?

**Es NORMAL en sistemas RAG generativos.**

```
Word Accuracy baja (0.22) + BERT alto (0.83) = âœ… REFORMULACIÃ“N CORRECTA

El sistema NO copia literalmente las referencias.
GENERA respuestas originales manteniendo el significado.
```

### Ejemplo Real

```
Referencia:
"Se utiliza el modelo paraphrase-multilingual-MiniLM-L12-v2 
de Sentence Transformers."

Respuesta del Sistema:
"El sistema utiliza sentence-transformers/paraphrase-multilingual-
MiniLM-L12-v2 para embeddings."

WER:  Alto (palabras diferentes) âš ï¸
BERT: Alto (mismo significado)   âœ…
â†’ CORRECTO: Reformula informaciÃ³n manteniendo precisiÃ³n
```

### Â¿CuÃ¡ndo preocuparse?

âŒ **Preocuparse si:**
- Word Acc baja **Y** BERT bajo (< 0.70)
- Errores factuales
- PÃ©rdida de informaciÃ³n tÃ©cnica

âœ… **No preocuparse si:**
- Word Acc baja **pero** BERT alto (> 0.80) â† **Nuestro caso**
- InformaciÃ³n correcta
- Detalles tÃ©cnicos preservados

---

## ğŸ¯ Casos de Uso

### 1. ValidaciÃ³n Pre-Deployment

```bash
# Ejecutar antes de desplegar
pytest tests/test_semantic_evaluation.py -v

# Verificar que pase umbrales
âœ… BERT F1 > 0.75
âœ… ROUGE-1 > 0.30
âœ… ROUGE-L > 0.25
```

### 2. Monitoreo Continuo

```bash
# Ejecutar semanalmente
pytest tests/test_semantic_evaluation.py::test_evaluacion_completa -v

# Comparar con baseline
# BERT: 0.8335 (baseline)
# ROUGE-1: 0.4558 (baseline)
```

### 3. RegresiÃ³n Testing

```bash
# DespuÃ©s de cambios en el modelo
pytest tests/test_semantic_evaluation.py -v

# Verificar que no empeoren mÃ©tricas
# BERT: Mantener > 0.80
# ROUGE-1: Mantener > 0.40
```

### 4. AnÃ¡lisis de Mejoras

```bash
# AÃ±adir casos al dataset
# Ejecutar pruebas
# Comparar mÃ©tricas antes/despuÃ©s
```

---

## ğŸ”§ ConfiguraciÃ³n

### Requisitos

```bash
Python 3.11+
pytest >= 9.0.1
bert-score >= 0.3.13
rouge-score >= 0.1.2
jiwer >= 4.0.0
```

### InstalaciÃ³n

```bash
# OpciÃ³n 1: requirements-test.txt
pip install -r requirements-test.txt

# OpciÃ³n 2: Individual
pip install bert-score rouge-score jiwer pytest
```

### Primera EjecuciÃ³n

```bash
# La primera vez descarga modelo BERT (~420MB)
pytest tests/test_semantic_evaluation.py -v

# Ejecuciones posteriores usan cache (mÃ¡s rÃ¡pido)
```

---

## ğŸ“ˆ Comandos Ãštiles

### Ejecutar Todas las Pruebas

```bash
pytest tests/test_semantic_evaluation.py -v
```

### Ejecutar por MÃ©trica

```bash
# Solo BERT Score
pytest tests/test_semantic_evaluation.py::test_bert_score_promedio -v

# Solo ROUGE
pytest tests/test_semantic_evaluation.py::test_rouge_promedio -v

# Solo WER
pytest tests/test_semantic_evaluation.py::test_wer_promedio -v
```

### Ejecutar Caso EspecÃ­fico

```bash
# Caso individual BERT
pytest tests/test_semantic_evaluation.py::test_bert_score_individual[caso_1] -v

# Caso individual ROUGE
pytest tests/test_semantic_evaluation.py::test_rouge_individual[caso_1] -v

# Caso individual WER
pytest tests/test_semantic_evaluation.py::test_wer_individual[caso_1] -v
```

### Generar Reporte Completo

```bash
pytest tests/test_semantic_evaluation.py::test_evaluacion_completa -v
cat reports/evaluacion_completa.txt
```

---

## ğŸ“Š Estructura de Reportes

```
reports/
â”œâ”€â”€ evaluacion_completa.txt      # ğŸ” Reporte completo con todos los casos
â”‚   â”œâ”€â”€ MÃ©tricas promedio
â”‚   â”œâ”€â”€ Resultados por caso
â”‚   â””â”€â”€ EvaluaciÃ³n cualitativa
â”‚
â”œâ”€â”€ bert_score_summary.txt       # ğŸ§  Resumen BERT Score
â”‚   â”œâ”€â”€ Precision promedio
â”‚   â”œâ”€â”€ Recall promedio
â”‚   â””â”€â”€ F1 promedio
â”‚
â”œâ”€â”€ rouge_summary.txt            # ğŸ“ Resumen ROUGE
â”‚   â”œâ”€â”€ ROUGE-1 F1
â”‚   â”œâ”€â”€ ROUGE-2 F1
â”‚   â””â”€â”€ ROUGE-L F1
â”‚
â””â”€â”€ wer_summary.txt              # ğŸ” Resumen WER
    â”œâ”€â”€ WER promedio
    â””â”€â”€ Word Accuracy promedio
```

---

## ğŸ“ InterpretaciÃ³n de Resultados

### Escala de Calidad

#### BERT F1 Score
```
0.90 - 1.00  ğŸŸ¢ğŸŸ¢  EXCELENTE    Significado casi idÃ©ntico
0.80 - 0.90  ğŸŸ¢    BUENA        Captura bien el significado  â† Aconex: 0.83
0.70 - 0.80  ğŸŸ¡    ACEPTABLE    Significado similar
< 0.70       ğŸ”´    POBRE        Significado diferente
```

#### ROUGE-1 F1
```
> 0.50       ğŸŸ¢ğŸŸ¢  ALTA         Excelente cobertura lÃ©xica
0.40 - 0.50  ğŸŸ¢    BUENA        Buen vocabulario            â† Aconex: 0.46
0.30 - 0.40  ğŸŸ¡    MEDIA        Vocabulario aceptable
< 0.30       ğŸ”´    BAJA         Vocabulario insuficiente
```

#### Word Accuracy
```
> 0.70       ğŸŸ¢ğŸŸ¢  ALTA         Casi copia exacta
0.50 - 0.70  ğŸŸ¢    MEDIA        ReformulaciÃ³n ligera
0.30 - 0.50  ğŸŸ¡    BAJA         ReformulaciÃ³n significativa
< 0.30       âš ï¸    MUY BAJA     Alto nivel de reformulaciÃ³n â† Aconex: 0.22
                                (Normal en RAG si BERT alto)
```

---

## ğŸš€ IntegraciÃ³n CI/CD

### GitHub Actions

```yaml
name: Semantic Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install bert-score rouge-score jiwer pytest
      - name: Run semantic tests
        run: |
          pytest tests/test_semantic_evaluation.py -v
      - name: Upload reports
        uses: actions/upload-artifact@v3
        with:
          name: semantic-reports
          path: reports/
```

### Umbrales de Calidad

```python
# En tests/test_semantic_evaluation.py
THRESHOLDS = {
    "bert_f1": 0.75,      # MÃ­nimo para aprobar
    "rouge1_f1": 0.30,    # Cobertura lÃ©xica mÃ­nima
    "rougeL_f1": 0.25,    # Estructura mÃ­nima
    "word_accuracy": 0.30 # Exactitud mÃ­nima (flexible en RAG)
}
```

---

## ğŸ’¡ Mejores PrÃ¡cticas

### 1. Mantener Dataset Actualizado

```python
# AÃ±adir casos cuando:
- Nueva feature en el sistema
- Nuevos tipos de consultas
- Cambios en el modelo
- Feedback de usuarios
```

### 2. Monitoreo Regular

```bash
# Ejecutar semanalmente
pytest tests/test_semantic_evaluation.py -v

# Comparar con baseline
# Alertar si BERT cae > 5%
```

### 3. AnÃ¡lisis de Casos Fallidos

```python
# Si un caso falla:
1. Revisar BERT vs WER
2. Si BERT alto â†’ No preocuparse
3. Si BERT bajo â†’ Investigar causa
```

### 4. Documentar Cambios

```markdown
# En cada actualizaciÃ³n:
- Nueva mÃ©trica baseline
- Casos aÃ±adidos/modificados
- Cambios en umbrales
- RazÃ³n de los cambios
```

---

## ğŸ”— Enlaces Relacionados

### DocumentaciÃ³n Interna
- [Pruebas de Capacidad](./PRUEBAS_CAPACIDAD.md)
- [Testing General](./TESTING_GUIDE.md)
- [DocumentaciÃ³n TÃ©cnica](./DOCUMENTACION_TESTS.md)

### Recursos Externos
- [BERT Score Paper](https://arxiv.org/abs/1904.09675)
- [ROUGE Metrics](https://aclanthology.org/W04-1013/)
- [WER Calculation](https://en.wikipedia.org/wiki/Word_error_rate)

### LibrerÃ­as
- [bert-score](https://github.com/Tiiiger/bert_score)
- [rouge-score](https://github.com/google-research/google-research/tree/master/rouge)
- [jiwer](https://github.com/jitsi/jiwer)

---

## ğŸ“ Soporte

### Problemas Comunes

**Error: ModuleNotFoundError**
```bash
pip install bert-score rouge-score jiwer pytest
```

**BERT Score lento**
```bash
# Usar GPU si disponible
export CUDA_VISIBLE_DEVICES=0
```

**Tests WER fallan**
```bash
# Revisar que BERT sea > 0.75
# WER bajo es normal en RAG
```

### Contacto

- ğŸ“§ Issues: GitHub Issues
- ğŸ“š Docs: Ver documentaciÃ³n completa
- ğŸ’¬ Preguntas: Revisar FAQ en documentos

---

## ğŸ“ Changelog

### v1.0.0 (Diciembre 2024)
- âœ… ImplementaciÃ³n inicial de pruebas semÃ¡nticas
- âœ… 8 casos de evaluaciÃ³n
- âœ… IntegraciÃ³n BERT, ROUGE, WER
- âœ… GeneraciÃ³n automÃ¡tica de reportes
- âœ… DocumentaciÃ³n completa en espaÃ±ol
- âœ… Sistema aprobado con BERT 0.83

---

## ğŸ¯ Resumen Ejecutivo

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      PRUEBAS SEMÃNTICAS - SISTEMA ACONEX RAG           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                        â•‘
â•‘  Estado:     âœ… APROBADO PARA PRODUCCIÃ“N              â•‘
â•‘  Calidad:    â­â­â­â­ (4/5 estrellas)                â•‘
â•‘                                                        â•‘
â•‘  BERT F1:    0.8335  âœ… Excelente comprensiÃ³n         â•‘
â•‘  ROUGE-1:    0.4558  âœ… Buen vocabulario              â•‘
â•‘  ROUGE-L:    0.4097  âœ… Buena estructura              â•‘
â•‘  Word Acc:   0.2237  âš ï¸ ReformulaciÃ³n (OK)           â•‘
â•‘                                                        â•‘
â•‘  Casos:      8 evaluados                               â•‘
â•‘  Tests:      28 ejecutados, 20 aprobados (71%)         â•‘
â•‘                                                        â•‘
â•‘  PrÃ³ximos pasos:                                       â•‘
â•‘  1. âœ… Desplegar en producciÃ³n                        â•‘
â•‘  2. ğŸ“‹ Configurar monitoreo continuo                  â•‘
â•‘  3. ğŸ“‹ Expandir dataset (20-30 casos)                 â•‘
â•‘  4. ğŸ“‹ Integrar feedback de usuarios                  â•‘
â•‘                                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

**Generado:** Diciembre 2024  
**VersiÃ³n:** 1.0.0  
**Mantenido por:** Equipo Aconex RAG  
**Ãšltima actualizaciÃ³n:** Diciembre 2024

---

## ğŸš€ Get Started

```bash
# 1ï¸âƒ£ Quick start (2 minutos)
cd backend-acorag
pip install bert-score rouge-score jiwer pytest
pytest tests/test_semantic_evaluation.py -v
cat reports/evaluacion_completa.txt

# 2ï¸âƒ£ Leer documentaciÃ³n (5 minutos)
# Ver RESUMEN_EJECUTIVO_SEMANTICAS.md

# 3ï¸âƒ£ Profundizar (30 minutos)
# Ver PRUEBAS_SEMANTICAS_RAG.md
```

**Â¡Listo para evaluar la calidad de tu RAG!** ğŸ‰
