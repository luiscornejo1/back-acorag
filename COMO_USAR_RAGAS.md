# ğŸ¯ CÃ³mo Evaluar Aconex RAG con RAGAS y SemScore

## ğŸ“Š Estado Actual

### âœ… Ya Implementado

1. **MÃ©tricas BÃ¡sicas NLP** (Completado)
   - BERT Score: 0.8335 âœ…
   - ROUGE-1: 0.4558 âœ…
   - Word Accuracy: 0.2237 âš ï¸ (Normal en RAG)
   - Archivo: `tests/test_semantic_evaluation.py`
   - Reportes: `reports/*.txt`

2. **Framework RAGAS** (Implementado, pendiente ejecutar)
   - Faithfulness (detecta alucinaciones)
   - Answer Relevancy (relevancia de respuestas)
   - Context Precision (calidad del retrieval)
   - Context Recall (completitud)
   - Answer Similarity (SemScore de Hugging Face)
   - Archivo: `tests/test_ragas_evaluation.py`

---

## ğŸš€ CÃ³mo Usar RAGAS (Paso a Paso)

### Paso 1: Obtener API Key de OpenAI

**Â¿Por quÃ© necesito esto?**
- RAGAS usa GPT-4 para evaluar las respuestas (mÃ¡s preciso que mÃ©tricas automÃ¡ticas)
- El costo es mÃ­nimo: ~$0.50 para 100 evaluaciones

**CÃ³mo obtenerla:**
1. Ve a https://platform.openai.com/api-keys
2. Crea una cuenta o inicia sesiÃ³n
3. Click en "Create new secret key"
4. Copia la key (empieza con `sk-...`)

### Paso 2: Configurar la API Key

**OpciÃ³n A: Variable de entorno (temporal)**
```powershell
# En PowerShell
$env:OPENAI_API_KEY="sk-tu-api-key-aqui"
```

**OpciÃ³n B: Archivo .env (permanente)**
```bash
# Crear archivo .env en backend-acorag/
cd backend-acorag
echo "OPENAI_API_KEY=sk-tu-api-key-aqui" > .env
```

### Paso 3: Ejecutar EvaluaciÃ³n RAGAS

```bash
cd backend-acorag

# OpciÃ³n 1: Script directo (mÃ¡s rÃ¡pido)
python tests/test_ragas_evaluation.py

# OpciÃ³n 2: Con pytest (mÃ¡s detallado)
pytest tests/test_ragas_evaluation.py -v
```

**Tiempo estimado:** 2-5 minutos (depende de API de OpenAI)

### Paso 4: Ver Resultados

```bash
# Reporte completo
cat reports/ragas_evaluation.txt

# CSV para anÃ¡lisis
cat reports/ragas_results.csv
```

---

## ğŸ“Š QuÃ© Esperar de los Resultados

### MÃ©tricas RAGAS Explicadas

#### 1. Faithfulness (Fidelidad)
```
Â¿QuÃ© mide? Â¿La respuesta es fiel a los documentos recuperados?
Objetivo: > 0.7
Detecta: Alucinaciones (sistema inventa informaciÃ³n)

Ejemplo:
Contexto: "El sistema usa PostgreSQL"
Respuesta: "El sistema usa PostgreSQL"     â†’ 1.0 âœ…
Respuesta: "El sistema usa MongoDB"        â†’ 0.0 âŒ (alucinaciÃ³n)
```

**Si es bajo (< 0.5):**
- El sistema estÃ¡ inventando informaciÃ³n
- AcciÃ³n: Ajustar prompt para ser mÃ¡s fiel a contexto

#### 2. Answer Relevancy (Relevancia)
```
Â¿QuÃ© mide? Â¿La respuesta es relevante a la pregunta?
Objetivo: > 0.7
Detecta: Respuestas off-topic

Ejemplo:
Pregunta: "Â¿QuÃ© base de datos usa?"
Respuesta: "PostgreSQL con pgvector"                    â†’ 1.0 âœ…
Respuesta: "El sistema tiene mÃºltiples componentes..."  â†’ 0.5 âš ï¸
```

**Si es bajo (< 0.5):**
- El sistema responde cosas no relacionadas
- AcciÃ³n: Mejorar prompt, ajustar retrieval

#### 3. Context Precision (PrecisiÃ³n del Retrieval)
```
Â¿QuÃ© mide? Â¿Los documentos recuperados son relevantes?
Objetivo: > 0.7
Detecta: Ruido en resultados de bÃºsqueda

Ejemplo:
Pregunta: "Â¿QuÃ© base de datos usa?"
Contextos recuperados:
  1. "PostgreSQL con pgvector"              â†’ Relevante âœ…
  2. "API endpoints disponibles"            â†’ No relevante âŒ
Context Precision = 0.5 (50% relevantes)
```

**Si es bajo (< 0.5):**
- El retrieval recupera muchos documentos irrelevantes
- AcciÃ³n: Ajustar threshold de similitud, mejorar embeddings

#### 4. Context Recall (Completitud)
```
Â¿QuÃ© mide? Â¿Se recuperaron TODOS los docs necesarios?
Objetivo: > 0.7
Detecta: InformaciÃ³n faltante

Ejemplo:
Ground truth menciona: "PostgreSQL", "pgvector", "embeddings"
Contextos recuperados mencionan: "PostgreSQL", "pgvector"
Context Recall = 0.67 (falta "embeddings")
```

**Si es bajo (< 0.5):**
- El retrieval no encuentra toda la informaciÃ³n necesaria
- AcciÃ³n: Aumentar k (nÃºmero de documentos recuperados)

#### 5. Answer Similarity (SemScore)
```
Â¿QuÃ© mide? Similitud semÃ¡ntica con respuesta ideal
Objetivo: > 0.7
Detecta: QuÃ© tan similar es a la respuesta perfecta

Es equivalente a BERT Score que ya implementamos.
Nuestro BERT Score: 0.8335 âœ…
```

---

## ğŸ¯ ComparaciÃ³n: MÃ©tricas BÃ¡sicas vs RAGAS

### Lo que ya tienes (MÃ©tricas BÃ¡sicas)

```
RESULTADOS ACTUALES:
âœ… BERT F1:        0.8335  (Excelente similitud semÃ¡ntica)
âœ… ROUGE-1:        0.4558  (Buena cobertura lÃ©xica)
âœ… ROUGE-L:        0.4097  (Buena estructura)
âš ï¸ Word Accuracy:  0.2237  (ReformulaciÃ³n - normal en RAG)

CONCLUSIÃ“N: El sistema COMPRENDE bien y usa vocabulario apropiado
```

**Limitaciones:**
- âŒ No evalÃºan el retrieval (solo la generaciÃ³n)
- âŒ No detectan alucinaciones
- âŒ No miden relevancia de respuestas

### Lo que vas a obtener (RAGAS)

```
RESULTADOS ESPERADOS:
âœ… Faithfulness:        0.85  (Sin alucinaciones)
âœ… Answer Relevancy:    0.82  (Respuestas relevantes)
âœ… Context Precision:   0.78  (Retrieval preciso)
âœ… Context Recall:      0.76  (Recupera info completa)
âœ… Answer Similarity:   0.83  (Similar a BERT Score)
```

**Ventajas:**
- âœ… EvalÃºa TODO el pipeline RAG (retrieval + generation)
- âœ… Detecta alucinaciones
- âœ… Mide calidad del retrieval
- âœ… MÃ¡s preciso (usa GPT-4 para evaluaciÃ³n)

---

## ğŸ’¡ InterpretaciÃ³n de Resultados

### Escenario 1: Todo estÃ¡ bien âœ…

```
Faithfulness:      0.85  âœ…
Answer Relevancy:  0.82  âœ…
Context Precision: 0.78  âœ…
Context Recall:    0.76  âœ…
Answer Similarity: 0.83  âœ…
```

**InterpretaciÃ³n:**
- El sistema funciona excelentemente
- No hay alucinaciones
- Retrieval es efectivo
- Respuestas son relevantes y similares a las ideales

**AcciÃ³n:** Mantener y monitorear

### Escenario 2: Problema de Alucinaciones âš ï¸

```
Faithfulness:      0.45  âŒ Bajo
Answer Relevancy:  0.80  âœ… OK
Context Precision: 0.75  âœ… OK
Context Recall:    0.70  âœ… OK
Answer Similarity: 0.50  âš ï¸ Bajo
```

**InterpretaciÃ³n:**
- El retrieval funciona bien
- PERO el sistema inventa informaciÃ³n no presente en documentos
- Las respuestas son relevantes pero incorrectas

**AcciÃ³n:**
1. Ajustar prompt: "Responde SOLO con informaciÃ³n del contexto"
2. Reducir temperatura del LLM (mÃ¡s conservador)
3. AÃ±adir validaciÃ³n: "Si no sabes, di 'No tengo esa informaciÃ³n'"

### Escenario 3: Problema de Retrieval âš ï¸

```
Faithfulness:      0.85  âœ… OK
Answer Relevancy:  0.80  âœ… OK
Context Precision: 0.40  âŒ Bajo
Context Recall:    0.35  âŒ Bajo
Answer Similarity: 0.70  âš ï¸ Moderado
```

**InterpretaciÃ³n:**
- El LLM genera bien (es fiel al contexto que recibe)
- PERO el retrieval no encuentra los documentos correctos
- Recupera documentos irrelevantes o incompletos

**AcciÃ³n:**
1. Mejorar embeddings: Usar modelo mÃ¡s grande o fine-tuned
2. Ajustar k: Recuperar mÃ¡s documentos (ej: k=5 en vez de k=3)
3. Ajustar threshold: Bajar umbral de similitud
4. Re-indexar documentos: Mejorar chunking

### Escenario 4: Problema de Relevancia âš ï¸

```
Faithfulness:      0.85  âœ… OK
Answer Relevancy:  0.45  âŒ Bajo
Context Precision: 0.75  âœ… OK
Context Recall:    0.70  âœ… OK
Answer Similarity: 0.60  âš ï¸ Moderado
```

**InterpretaciÃ³n:**
- Retrieval funciona
- Respuestas son fieles
- PERO respuestas no son directas (mucha info extra)

**AcciÃ³n:**
1. Mejorar prompt: "Responde directamente y de forma concisa"
2. Ajustar system message
3. Post-procesar respuestas para hacerlas mÃ¡s directas

---

## ğŸ“ˆ Flujo de Trabajo Recomendado

### 1. Primera EvaluaciÃ³n (Ahora)

```bash
# Ejecutar mÃ©tricas bÃ¡sicas (ya hecho)
pytest tests/test_semantic_evaluation.py -v

# Ejecutar RAGAS (nuevo)
python tests/test_ragas_evaluation.py
```

**Resultado:** Baseline para comparar futuras mejoras

### 2. Identificar Problemas

```bash
# Ver reportes
cat reports/ragas_evaluation.txt

# Identificar mÃ©tricas bajas
# Si Faithfulness < 0.7 â†’ Problema de alucinaciones
# Si Context Precision < 0.7 â†’ Problema de retrieval
# Si Answer Relevancy < 0.7 â†’ Problema de generaciÃ³n
```

### 3. Implementar Mejoras

SegÃºn los problemas identificados:
- Ajustar prompts
- Mejorar embeddings
- Cambiar parÃ¡metros (k, threshold)
- Re-indexar documentos

### 4. Re-evaluar

```bash
# Ejecutar de nuevo
python tests/test_ragas_evaluation.py

# Comparar con baseline
# Â¿Las mÃ©tricas mejoraron?
```

### 5. Iterar

Repetir pasos 2-4 hasta alcanzar objetivos

---

## ğŸ¯ Objetivos de MÃ©tricas

### MÃ­nimo (Pre-producciÃ³n)
```
Faithfulness:      > 0.70  âœ…
Answer Relevancy:  > 0.70  âœ…
Context Precision: > 0.60  âš ï¸ (Puede haber algo de ruido)
Context Recall:    > 0.60  âš ï¸ (Puede faltar alguna info)
Answer Similarity: > 0.70  âœ…
```

### Ideal (ProducciÃ³n)
```
Faithfulness:      > 0.85  ğŸ¯
Answer Relevancy:  > 0.80  ğŸ¯
Context Precision: > 0.75  ğŸ¯
Context Recall:    > 0.75  ğŸ¯
Answer Similarity: > 0.80  ğŸ¯
```

### Excelente (Estado del arte)
```
Faithfulness:      > 0.90  â­
Answer Relevancy:  > 0.85  â­
Context Precision: > 0.85  â­
Context Recall:    > 0.85  â­
Answer Similarity: > 0.85  â­
```

---

## ğŸ”§ Troubleshooting

### "OPENAI_API_KEY not configured"

```powershell
# Verificar
$env:OPENAI_API_KEY

# Si estÃ¡ vacÃ­o, configurar
$env:OPENAI_API_KEY="sk-tu-key-aqui"
```

### "Rate limit exceeded"

```python
# OpciÃ³n 1: Usar modelo mÃ¡s barato
llm = ChatOpenAI(model="gpt-4o-mini")  # En vez de gpt-4

# OpciÃ³n 2: Reducir dataset
# Editar tests/test_ragas_evaluation.py
# Usar solo 3-5 casos en vez de 8
```

### EvaluaciÃ³n muy lenta

```python
# Evaluar solo mÃ©tricas crÃ­ticas
metricas = [
    faithfulness,      # MÃ¡s importante
    answer_relevancy,  # TambiÃ©n crÃ­tica
]
```

---

## ğŸ“š DocumentaciÃ³n Generada

### Archivos Creados

1. **`tests/test_ragas_evaluation.py`** (590 lÃ­neas)
   - Script completo de evaluaciÃ³n RAGAS
   - 6 mÃ©tricas implementadas
   - Tests individuales por mÃ©trica

2. **`INICIO_RAPIDO_RAGAS.md`**
   - GuÃ­a rÃ¡pida para ejecutar
   - InterpretaciÃ³n de mÃ©tricas
   - Comandos Ãºtiles

3. **`COMO_USAR_RAGAS.md`** (este archivo)
   - GuÃ­a completa paso a paso
   - Comparaciones y ejemplos
   - Troubleshooting

### Reportes que se Generan

DespuÃ©s de ejecutar, se crean:
- `reports/ragas_evaluation.txt` - Reporte completo
- `reports/ragas_results.csv` - Datos para anÃ¡lisis

---

## ğŸ‰ PrÃ³ximos Pasos

### Inmediato (Hoy)
1. âœ… Configurar OPENAI_API_KEY
2. âœ… Ejecutar `python tests/test_ragas_evaluation.py`
3. âœ… Revisar `reports/ragas_evaluation.txt`

### Corto Plazo (Esta semana)
4. ğŸ“‹ Comparar resultados RAGAS con mÃ©tricas bÃ¡sicas
5. ğŸ“‹ Identificar Ã¡reas de mejora
6. ğŸ“‹ Documentar hallazgos

### Mediano Plazo (PrÃ³ximo mes)
7. ğŸ“‹ Implementar mejoras basadas en resultados
8. ğŸ“‹ Re-evaluar y comparar
9. ğŸ“‹ Integrar en CI/CD

---

## ğŸ’° Costo Estimado

**RAGAS con OpenAI API:**
- Modelo: gpt-4o-mini (mÃ¡s barato)
- Costo: ~$0.15 por token output
- EvaluaciÃ³n completa (8 casos): ~$0.30-0.50
- 100 evaluaciones: ~$5-10

**ComparaciÃ³n:**
- MÃ©tricas bÃ¡sicas (BERT, ROUGE): Gratis pero menos precisas
- RAGAS: Bajo costo pero muy preciso

**RecomendaciÃ³n:**
- Desarrollo: Usa mÃ©tricas bÃ¡sicas (gratis, rÃ¡pido)
- Pre-deployment: Usa RAGAS (preciso, detecta problemas)
- ProducciÃ³n: Combina ambos (CI con bÃ¡sicas, audit con RAGAS)

---

## ğŸ”— Referencias

- [RAGAS DocumentaciÃ³n](https://docs.ragas.io/)
- [Paper RAGAS](https://arxiv.org/abs/2309.15217)
- [SemScore Hugging Face](https://huggingface.co/spaces/evaluate-metric/semscore)
- [MÃ©tricas BÃ¡sicas](./PRUEBAS_SEMANTICAS_RAG.md)

---

**Generado:** Diciembre 2024  
**Estado:** âœ… Listo para ejecutar  
**PrÃ³ximo paso:** Configurar OPENAI_API_KEY y correr evaluaciÃ³n
