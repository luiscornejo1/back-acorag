# üìö Explicaci√≥n T√©cnica Detallada de Tests - Sistema RAG Aconex

## üéØ Objetivo del Documento

Este documento proporciona una explicaci√≥n t√©cnica exhaustiva de cada test implementado, incluyendo:
- C√≥digo espec√≠fico que se est√° testeando
- Tecnolog√≠as y librer√≠as utilizadas
- Patrones de dise√±o aplicados
- Assertions y validaciones t√©cnicas
- Razones arquitect√≥nicas de cada test

---

## üì¶ Stack Tecnol√≥gico de Testing

```python
# Framework de Testing
pytest==9.0.1           # Framework principal de testing
pytest-asyncio==1.3.0   # Soporte para tests as√≠ncronos
pytest-mock==3.15.1     # Mocking y patching
pytest-cov==7.0.0       # An√°lisis de cobertura de c√≥digo

# Librer√≠as del Sistema
psycopg2-binary         # Driver PostgreSQL
sentence-transformers   # Modelos de embeddings (768 dims)
numpy                   # Arrays para vectores
fastapi                 # Framework web para endpoints
pydantic                # Validaci√≥n de datos
groq                    # Cliente LLM para generaci√≥n
```

---

## üß™ ESCENARIO 1: INGESTA DE DATOS

### Test 1.1: `test_normalize_doc_complete` (Positivo)

**üéØ Objetivo:** Validar la normalizaci√≥n completa de documentos Aconex con todos sus metadatos.

**üìç C√≥digo bajo test:**
```python
# Archivo: app/ingest.py
def normalize_doc(doc: Dict[str, Any], default_project_id: str = None) -> Dict[str, Any]:
    """
    Normaliza un documento Aconex extrayendo metadatos y construyendo body_text.
    
    Estructura de entrada (doc):
    {
        "DocumentId": "200076-CCC02-PL-AR-000400",
        "project_id": "PROJ-TEST-001",
        "metadata": {
            "Title": "Plan Maestro",
            "Number": "200076-CCC02-PL-AR-000400",
            "Category": "Arquitectura",
            "DocumentType": "Plano",
            "Status": "Aprobado",
            "ReviewStatus": "Revisado",
            "Revision": "Rev 3",
            "Filename": "plan_maestro.pdf",
            "FileType": "pdf",
            "FileSize": 2548736,
            "DateModified": "2024-01-15T10:30:00Z"
        },
        "full_text": "Contenido t√©cnico del documento..."
    }
    
    Transformaciones aplicadas:
    1. Extracci√≥n de metadatos desde doc["metadata"]
    2. Priorizaci√≥n de project_id (nivel superior > metadata)
    3. Construcci√≥n de body_text = f"{title}\n\n{full_text}"
    4. Parseo de fecha ISO 8601 a datetime
    """
    
    # 1. Extracci√≥n de project_id (prioridad: nivel superior)
    project_id = doc.get("project_id") or doc.get("metadata", {}).get("ProjectId", default_project_id)
    
    # 2. Extracci√≥n de metadatos
    metadata = doc.get("metadata", {})
    title = metadata.get("Title", "")
    number = metadata.get("Number", "")
    category = metadata.get("Category", "")
    doc_type = metadata.get("DocumentType", "")
    status = metadata.get("Status", "")
    review_status = metadata.get("ReviewStatus", "")
    revision = metadata.get("Revision", "")
    filename = metadata.get("Filename", "")
    file_type = metadata.get("FileType", "")
    file_size = metadata.get("FileSize", 0)
    
    # 3. Construcci√≥n de body_text para embeddings
    full_text = doc.get("full_text", "")
    body_text = f"{title}\n\n{full_text}" if full_text else title
    
    # 4. Parseo de fecha (ISO 8601 ‚Üí datetime)
    date_str = metadata.get("DateModified", "")
    date_modified = datetime.fromisoformat(date_str.replace("Z", "+00:00")) if date_str else None
    
    return {
        "document_id": doc.get("DocumentId"),
        "project_id": project_id,
        "title": title,
        "number": number,
        "category": category,
        "doc_type": doc_type,
        "status": status,
        "review_status": review_status,
        "revision": revision,
        "filename": filename,
        "file_type": file_type,
        "file_size": file_size,
        "body_text": body_text,
        "date_modified": date_modified
    }
```

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_normalize_doc_complete(sample_aconex_document):
    # Arrange: Fixture proporciona documento completo
    # sample_aconex_document es un fixture definido en conftest.py
    
    # Act: Ejecutar normalizaci√≥n
    result = normalize_doc(sample_aconex_document, "DEFAULT-PROJ")
    
    # Assert: Validaciones t√©cnicas
    assert result["document_id"] == "200076-CCC02-PL-AR-000400"
    assert result["project_id"] == "PROJ-TEST-001"  # Nivel superior tiene prioridad
    assert result["title"] == "Plan Maestro de Arquitectura"
    assert result["file_size"] == 2548736  # Tipo int preservado
    
    # Validaci√≥n de tipo datetime (no string)
    assert isinstance(result["date_modified"], datetime)
    assert result["date_modified"].year == 2024
    assert result["date_modified"].month == 1
    assert result["date_modified"].day == 15
    
    # Validaci√≥n de construcci√≥n de body_text
    assert "Plan Maestro" in result["body_text"]
    assert len(result["body_text"]) > 100  # Contenido sustancial
```

**üîç Assertions T√©cnicas:**
1. **Extracci√≥n correcta de metadatos**: Verifica que cada campo se extraiga de la estructura anidada
2. **Prioridad de project_id**: `doc["project_id"]` > `doc["metadata"]["ProjectId"]` > `default_project_id`
3. **Parseo de fecha ISO 8601**: String ‚Üí `datetime` object (zona horaria UTC)
4. **Construcci√≥n de body_text**: Concatenaci√≥n `title + "\n\n" + full_text`
5. **Preservaci√≥n de tipos**: `file_size` debe ser `int`, no `str`

---

### Test 1.2: `test_iter_docs_from_file_json_and_ndjson` (Positivo)

**üéØ Objetivo:** Validar lectura de formatos JSON array y NDJSON (newline-delimited JSON).

**üìç C√≥digo bajo test:**
```python
# Archivo: app/ingest.py
def iter_docs_from_file(json_path: str) -> Generator[Dict, None, None]:
    """
    Generador que itera sobre documentos en archivo JSON o NDJSON.
    
    Soporta 2 formatos:
    1. JSON Array: [{"doc": 1}, {"doc": 2}]
    2. NDJSON: {"doc": 1}\n{"doc": 2}\n
    
    Algoritmo de detecci√≥n:
    - Intenta json.load() ‚Üí Si √©xito, es JSON Array
    - Si falla, intenta l√≠nea por l√≠nea ‚Üí Es NDJSON
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        try:
            # Intento 1: Leer como JSON Array
            data = json.load(f)
            if isinstance(data, list):
                for doc in data:
                    yield doc
            else:
                yield data
        except json.JSONDecodeError:
            # Intento 2: Leer como NDJSON (l√≠nea por l√≠nea)
            f.seek(0)  # Resetear puntero de archivo
            for line in f:
                line = line.strip()
                if line:  # Ignorar l√≠neas vac√≠as
                    yield json.loads(line)
```

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_iter_docs_from_file_json_and_ndjson(tmp_path):
    # Test 1: JSON Array
    json_file = tmp_path / "docs_list.json"
    docs_list = [
        {"DocumentId": "001", "metadata": {"Title": "Doc 1"}},
        {"DocumentId": "002", "metadata": {"Title": "Doc 2"}}
    ]
    with open(json_file, 'w') as f:
        json.dump(docs_list, f)  # Escribe JSON array v√°lido
    
    result_list = list(iter_docs_from_file(str(json_file)))
    
    # Assertions: Validar parsing de JSON array
    assert len(result_list) == 2
    assert result_list[0]["DocumentId"] == "001"
    assert result_list[1]["DocumentId"] == "002"
    
    # Test 2: NDJSON (newline-delimited)
    ndjson_file = tmp_path / "docs.ndjson"
    with open(ndjson_file, 'w') as f:
        f.write('{"DocumentId": "003", "metadata": {"Title": "Doc 3"}}\n')
        f.write('\n')  # L√≠nea vac√≠a (debe ignorarse)
        f.write('{"DocumentId": "004", "metadata": {"Title": "Doc 4"}}\n')
    
    result_ndjson = list(iter_docs_from_file(str(ndjson_file)))
    
    # Assertions: Validar parsing de NDJSON + l√≠neas vac√≠as
    assert len(result_ndjson) == 2  # L√≠nea vac√≠a ignorada
    assert result_ndjson[0]["DocumentId"] == "003"
    assert result_ndjson[1]["DocumentId"] == "004"
```

**üîç Assertions T√©cnicas:**
1. **Detecci√≥n autom√°tica de formato**: No requiere par√°metro de formato
2. **Parsing de JSON array**: `json.load()` lee estructura completa
3. **Parsing l√≠nea por l√≠nea**: `json.loads(line)` para cada l√≠nea no vac√≠a
4. **Manejo de l√≠neas vac√≠as**: `if line.strip()` ignora l√≠neas vac√≠as en NDJSON
5. **Generator pattern**: Uso de `yield` para eficiencia de memoria

---

### Test 1.3: `test_normalize_doc_missing_fields` (Negativo)

**üéØ Objetivo:** Validar robustez ante documentos con campos faltantes.

**üìç C√≥digo bajo test:**
```python
# En normalize_doc(), uso de .get() con defaults
body_text = doc.get('body_text', '') or doc.get('subject', '')
from_company = doc.get('from_company', None)  # None es v√°lido
to_company = doc.get('to_company', None)
```

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_normalize_doc_missing_fields():
    incomplete_doc = {
        "project_id": "PROYECTO-001",
        "subject": "Documento sin metadata completa"
        # Faltan: body, from_company, to_company, date_sent
    }
    
    # Act: NO debe lanzar KeyError
    result = normalize_doc(incomplete_doc, "DEFAULT-PROJ")
    
    # Assertions: Verificar valores por defecto
    assert result is not None
    assert result["project_id"] == "PROYECTO-001"
    assert "from_company" in result  # Key existe (valor puede ser None)
    assert "to_company" in result
```

**üîç Assertions T√©cnicas:**
1. **Uso de `.get()` en lugar de `[]`**: Evita `KeyError`
2. **Valores por defecto**: `None` para opcionales, `""` para strings
3. **Graceful degradation**: Documento parcial es v√°lido

---

### Test 1.4: `test_iter_docs_invalid_json` (Negativo)

**üéØ Objetivo:** Validar que JSON malformado lance excepci√≥n clara.

**üìç C√≥digo bajo test:**
```python
# json.load() lanzar√° JSONDecodeError si JSON est√° corrupto
data = json.load(f)  # Puede lanzar json.JSONDecodeError
```

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_iter_docs_invalid_json(tmp_path):
    bad_json_file = tmp_path / "malformed.json"
    bad_json_file.write_text('{"subject": "incomplete"', encoding='utf-8')  # JSON incompleto
    
    # Assertion: Debe lanzar excepci√≥n
    with pytest.raises(Exception):  # JSONDecodeError hereda de Exception
        list(iter_docs_from_file(str(bad_json_file)))
```

**üîç Assertions T√©cnicas:**
1. **Fail-fast**: Sistema no debe ignorar JSON corrupto
2. **Excepci√≥n espec√≠fica**: `json.JSONDecodeError` con mensaje descriptivo
3. **Prevenci√≥n de data corruption**: Mejor fallar que procesar datos incorrectos

---

## üîç ESCENARIO 2: B√öSQUEDA SEM√ÅNTICA

### Test 2.1: `test_semantic_search_basic` (Positivo)

**üéØ Objetivo:** Validar flujo completo de b√∫squeda sem√°ntica vectorial con ranking h√≠brido.

**üìç C√≥digo bajo test:**
```python
# Archivo: app/search_core.py
def semantic_search(
    query: str,
    project_id: Optional[str] = None,
    top_k: int = 10,
    probes: int = 10
) -> List[Dict]:
    """
    B√∫squeda sem√°ntica usando embeddings vectoriales + b√∫squeda full-text.
    
    Flujo t√©cnico:
    1. Generar embedding de query (768 dims)
    2. Configurar probes de √≠ndice IVFFlat
    3. Ejecutar b√∫squeda vectorial con operador <=>
    4. Calcular score h√≠brido: (1 - cosine_distance) * 0.7 + bm25_score * 0.3
    5. Deduplicar por document_id (solo chunk m√°s relevante)
    """
    
    # 1. Generar embedding usando SentenceTransformer
    model = get_model()  # 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
    query_embedding = model.encode([query])[0]  # Shape: (768,)
    embedding_str = encode_vec_str(query_embedding)  # Formato PostgreSQL: '[0.123, -0.456, ...]'
    
    # 2. Configurar IVFFlat probes (afecta recall vs latencia)
    conn = get_conn()
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute(f"SET ivfflat.probes = {probes}")  # Mayor probes = mayor recall
        
        # 3. SQL con operador de distancia coseno
        sql = """
            SELECT 
                dc.document_id,
                d.title,
                d.number,
                d.category,
                d.doc_type,
                d.revision,
                d.filename,
                d.file_type,
                d.date_modified,
                dc.chunk_text AS snippet,
                (1 - (dc.embedding <=> %s::vector)) AS vector_score,  -- Operador <=> = distancia coseno
                ts_rank(to_tsvector('spanish', dc.chunk_text), plainto_tsquery('spanish', %s)) AS text_score
            FROM document_chunks dc
            JOIN documents d ON dc.document_id = d.id
            WHERE (%s IS NULL OR d.project_id = %s)  -- Filtro opcional por proyecto
            ORDER BY 
                (1 - (dc.embedding <=> %s::vector)) * 0.7 +  -- 70% peso vectorial
                ts_rank(to_tsvector('spanish', dc.chunk_text), plainto_tsquery('spanish', %s)) * 0.3  -- 30% peso textual
                DESC
            LIMIT %s
        """
        
        # 4. Ejecutar query
        cur.execute(sql, (
            embedding_str, query,  # Para vector_score y text_score
            project_id, project_id,  # Para filtro WHERE
            embedding_str, query,  # Para ORDER BY
            top_k
        ))
        
        results = cur.fetchall()
        
        # 5. Deduplicaci√≥n por document_id (solo chunk m√°s relevante)
        seen_docs = set()
        deduplicated = []
        for row in results:
            if row['document_id'] not in seen_docs:
                seen_docs.add(row['document_id'])
                deduplicated.append(dict(row))
        
        return deduplicated
```

**üß™ Test Implementation:**
```python
@pytest.mark.integration
@pytest.mark.db
@pytest.mark.mock
def test_semantic_search_basic(mock_model_loader, mock_db_connection):
    # Arrange: Mock del modelo de embeddings
    mock_model = mock_model_loader
    mock_embedding = np.random.rand(768).astype('float32')  # Vector 768D
    mock_model.encode.return_value = [mock_embedding]
    
    # Mock de resultados de BD
    mock_cursor = mock_db_connection.cursor.return_value.__enter__.return_value
    mock_results = [
        {
            "document_id": "DOC-001",
            "title": "Manual de Construcci√≥n S√≠smica",
            "snippet": "Normas NSR-10 para construcci√≥n sismo-resistente...",
            "vector_score": 0.92,  # Similitud coseno alta
            "text_score": 0.45,    # Score BM25
            "score": 0.78          # H√≠brido: 0.92*0.7 + 0.45*0.3
        }
    ]
    mock_cursor.fetchall.return_value = mock_results
    
    # Act: Ejecutar b√∫squeda
    with patch('app.search_core.get_conn', return_value=mock_db_connection):
        results = semantic_search(
            query="construcci√≥n sismo resistente",
            project_id=None,
            top_k=10,
            probes=10
        )
    
    # Assert: Validaciones t√©cnicas
    
    # 1. Verificar generaci√≥n de embedding
    assert mock_model.encode.called
    query_arg = mock_model.encode.call_args[0][0]
    assert isinstance(query_arg, list)
    assert query_arg[0] == "construcci√≥n sismo resistente"
    
    # 2. Verificar ejecuci√≥n de SQL
    execute_calls = mock_cursor.execute.call_args_list
    assert len(execute_calls) >= 2  # SET probes + SELECT
    
    # Verificar SET ivfflat.probes
    probes_sql = str(execute_calls[0][0][0])
    assert "ivfflat.probes" in probes_sql
    
    # Verificar operador de distancia vectorial
    main_sql = str(execute_calls[1][0][0])
    assert "<=>" in main_sql  # Operador de distancia coseno
    assert "vector_score" in main_sql
    assert "text_score" in main_sql
    
    # 3. Verificar estructura de resultados
    assert len(results) == 1
    assert results[0]["document_id"] == "DOC-001"
    assert 0 <= results[0]["vector_score"] <= 1
    assert results[0]["score"] >= results[0]["vector_score"] * 0.7  # Peso m√≠nimo
```

**üîç Assertions T√©cnicas:**
1. **Embedding generation**: `model.encode([query])` ‚Üí numpy array (768,)
2. **IVFFlat configuration**: `SET ivfflat.probes = N` afecta recall
3. **Operador `<=>`**: Distancia coseno en PostgreSQL con pgvector
4. **Score h√≠brido**: Combinaci√≥n 70% vectorial + 30% textual
5. **Deduplicaci√≥n**: Solo 1 chunk por document_id

---

### Test 2.2: `test_semantic_search_with_project_filter` (Positivo)

**üéØ Objetivo:** Validar aislamiento de datos multi-tenant por project_id.

**üìç C√≥digo bajo test:**
```python
# Cl√°usula WHERE con filtro de proyecto
WHERE (%s IS NULL OR d.project_id = %s)
```

**üß™ Test Implementation:**
```python
@pytest.mark.integration
def test_semantic_search_with_project_filter(mock_model_loader, mock_db_connection):
    # Arrange: Resultados solo de un proyecto
    mock_cursor = mock_db_connection.cursor.return_value.__enter__.return_value
    mock_results = [
        {"document_id": "EDU-001", "project_id": "PROYECTO-EDUCATIVO", ...}
    ]
    mock_cursor.fetchall.return_value = mock_results
    
    # Act: Buscar con filtro de proyecto
    with patch('app.search_core.get_conn', return_value=mock_db_connection):
        results = semantic_search(
            query="arquitectura educativa",
            project_id="PROYECTO-EDUCATIVO",  # ‚Üê Filtro cr√≠tico
            top_k=20
        )
    
    # Assert: Verificar SQL con filtro
    execute_calls = mock_cursor.execute.call_args_list
    main_query = execute_calls[1]
    sql_params = main_query[0][1]  # Par√°metros SQL
    
    assert "PROYECTO-EDUCATIVO" in sql_params
    
    # Verificar que TODOS los resultados pertenecen al proyecto
    for result in results:
        assert result["project_id"] == "PROYECTO-EDUCATIVO"
```

**üîç Assertions T√©cnicas:**
1. **Multi-tenancy**: Filtro `WHERE d.project_id = %s` en SQL
2. **SQL parametrizado**: Uso de `%s` previene SQL injection
3. **Aislamiento de datos**: Resultados solo del proyecto especificado

---

### Test 2.3: `test_semantic_search_empty_query` (Negativo)

**üéØ Objetivo:** Validar manejo de query vac√≠a sin crasheo.

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_semantic_search_empty_query(mock_model_loader, mock_db_connection):
    with patch('app.search_core.get_conn', return_value=mock_db_connection):
        results_empty = semantic_search(query="", project_id=None, top_k=10)
        
        # Assertions: NO debe crashear
        assert results_empty is not None
        assert isinstance(results_empty, list)
```

**üîç Assertions T√©cnicas:**
1. **Robustez**: Sistema no crashea con input vac√≠o
2. **Graceful handling**: Retorna lista vac√≠a o resultados generales

---

### Test 2.4: `test_semantic_search_invalid_project_id` (Negativo)

**üéØ Objetivo:** Validar comportamiento con proyecto inexistente.

**üß™ Test Implementation:**
```python
def test_semantic_search_invalid_project_id(mock_model_loader, mock_db_connection):
    # Arrange: BD retorna vac√≠o (proyecto no existe)
    mock_cursor = mock_db_connection.cursor.return_value.__enter__.return_value
    mock_cursor.fetchall.return_value = []
    
    # Act: Buscar en proyecto inexistente
    with patch('app.search_core.get_conn', return_value=mock_db_connection):
        results = semantic_search(
            query="test query",
            project_id="PROYECTO-INEXISTENTE-99999",
            top_k=10
        )
    
    # Assert: Retorna lista vac√≠a, NO error
    assert results == []
```

**üîç Assertions T√©cnicas:**
1. **Seguridad**: No revelar existencia de proyectos con error
2. **Comportamiento consistente**: Retornar `[]` es v√°lido

---

## üì§ ESCENARIO 3: UPLOAD Y PROCESAMIENTO

### Test 3.1: `test_extract_text_from_txt` (Positivo)

**üéØ Objetivo:** Validar extracci√≥n b√°sica de texto plano UTF-8.

**üìç C√≥digo bajo test:**
```python
# Archivo: app/upload.py
class DocumentUploader:
    def extract_text_from_txt(self, file_path: str) -> str:
        """
        Extrae texto de archivo TXT con encoding UTF-8.
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except Exception as e:
            raise Exception(f"Error leyendo archivo TXT: {e}")
```

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_extract_text_from_txt(tmp_path, mock_model_loader):
    # Arrange: Crear archivo TXT de prueba
    txt_file = tmp_path / "documento.txt"
    contenido = "Manual de Seguridad\n\nProcedimientos EPP..."
    txt_file.write_text(contenido, encoding='utf-8')
    
    # Act: Extraer texto
    uploader = DocumentUploader()
    result = uploader.extract_text_from_txt(str(txt_file))
    
    # Assert: Validar lectura UTF-8
    assert "Seguridad" in result
    assert "procedimientos" in result.lower()
    assert len(result) > 50
```

**üîç Assertions T√©cnicas:**
1. **Encoding UTF-8**: Soporte para caracteres especiales (√±, √°, etc.)
2. **`.strip()`**: Elimina whitespace al inicio/final
3. **Preservaci√≥n de contenido**: Texto completo sin p√©rdida

---

### Test 3.2: `test_generate_document_id_unique` (Positivo)

**üéØ Objetivo:** Validar generaci√≥n de IDs √∫nicos con formato MD5.

**üìç C√≥digo bajo test:**
```python
def generate_document_id(self, filename: str, content: str) -> str:
    """
    Genera ID √∫nico usando MD5(filename + content + timestamp).
    """
    import hashlib
    from datetime import datetime
    
    timestamp = datetime.now().isoformat()
    data = f"{filename}{content}{timestamp}".encode('utf-8')
    return hashlib.md5(data).hexdigest()  # 32 caracteres hex
```

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_generate_document_id_unique(mock_model_loader):
    uploader = DocumentUploader()
    filename = "manual.txt"
    content = "Contenido del documento"
    
    # Act: Generar ID
    id1 = uploader.generate_document_id(filename, content)
    
    # Assert: Validar formato MD5
    assert len(id1) == 32  # MD5 = 128 bits = 32 hex chars
    assert all(c in '0123456789abcdef' for c in id1)  # Solo hex
    
    # Validar unicidad con cambios
    id2 = uploader.generate_document_id(filename, content + " modificado")
    assert id2 != id1
    
    id3 = uploader.generate_document_id("otro.txt", content)
    assert id3 != id1
```

**üîç Assertions T√©cnicas:**
1. **MD5 hash**: 128 bits = 32 caracteres hexadecimales
2. **Unicidad**: Cambios en filename/content generan IDs diferentes
3. **Timestamp**: `datetime.now()` garantiza unicidad temporal

---

### Test 3.3: `test_extract_text_file_not_found` (Negativo)

**üéØ Objetivo:** Validar que archivo inexistente lance `FileNotFoundError`.

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_extract_text_file_not_found(mock_model_loader):
    uploader = DocumentUploader()
    nonexistent_file = "c:/archivos/que/no/existe.txt"
    
    # Assert: Debe lanzar FileNotFoundError
    with pytest.raises(FileNotFoundError):
        uploader.extract_text_from_txt(nonexistent_file)
```

**üîç Assertions T√©cnicas:**
1. **Excepci√≥n espec√≠fica**: `FileNotFoundError`, no gen√©rica
2. **Fail-fast**: Error inmediato, no procesamiento inv√°lido

---

### Test 3.4: `test_extract_text_invalid_encoding` (Negativo)

**üéØ Objetivo:** Validar manejo de archivos con encoding corrupto.

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_extract_text_invalid_encoding(tmp_path, mock_model_loader):
    # Arrange: Crear archivo binario inv√°lido para UTF-8
    bad_file = tmp_path / "corrupto.txt"
    bad_file.write_bytes(b'\x80\x81\x82\x83\xFF\xFE')
    
    uploader = DocumentUploader()
    
    # Assert: Puede lanzar UnicodeDecodeError o manejarlo
    try:
        result = uploader.extract_text_from_txt(str(bad_file))
        assert result is not None  # Si maneja internamente
    except UnicodeDecodeError:
        pass  # Excepci√≥n v√°lida
```

**üîç Assertions T√©cnicas:**
1. **Encoding errors**: Bytes `0x80-0xFF` inv√°lidos en UTF-8
2. **Dos comportamientos v√°lidos**: Lanzar excepci√≥n O manejar con reemplazo

---

## üí¨ ESCENARIO 4: CHAT RAG (Retrieval-Augmented Generation)

### Test 4.1: `test_chat_with_document_context` (Positivo)

**üéØ Objetivo:** Validar flujo completo RAG desde query hasta respuesta con LLM.

**üìç C√≥digo bajo test:**
```python
# Archivo: app/api.py
@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest) -> ChatResponse:
    """
    Endpoint de chat conversacional con RAG.
    
    Flujo t√©cnico:
    1. RETRIEVAL: B√∫squeda sem√°ntica de documentos
    2. FILTERING: Filtrar por score > 0.20
    3. AUGMENTATION: Construir contexto con documentos relevantes
    4. GENERATION: LLM (Groq) genera respuesta basada en contexto
    5. CITATION: Incluir fuentes para trazabilidad
    """
    
    # 1. B√∫squeda sem√°ntica
    search_results = semantic_search(
        query=req.question,
        project_id=req.project_id,
        top_k=req.max_context_docs
    )
    
    # 2. Filtrar por relevancia
    relevant_docs = [doc for doc in search_results if doc['score'] > 0.20]
    
    if not relevant_docs:
        return ChatResponse(
            question=req.question,
            answer="No encuentro informaci√≥n relevante en los documentos disponibles.",
            sources=[],
            context_used="",
            session_id=req.session_id
        )
    
    # 3. Construir contexto
    context_parts = []
    for doc in relevant_docs[:req.max_context_docs]:
        context_parts.append(f"[{doc['title']}]\n{doc['snippet']}")
    
    context = "\n\n---\n\n".join(context_parts)
    
    # 4. Llamar a LLM (Groq)
    from groq import Groq
    client = Groq(api_key=os.environ.get("GROQ_API_KEY"))
    
    prompt = f"""Bas√°ndote SOLO en estos documentos t√©cnicos:

{context}

Responde la pregunta: {req.question}

Si la informaci√≥n no est√° en los documentos, dilo expl√≠citamente.
Incluye referencias a los documentos citados."""
    
    completion = client.chat.completions.create(
        model="llama-3.1-70b-versatile",  # Groq LLM
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,  # Baja temperatura para respuestas factuales
        max_tokens=1024
    )
    
    answer = completion.choices[0].message.content
    
    # 5. Preparar sources
    sources = [
        {
            "id": doc['document_id'],
            "title": doc['title'],
            "score": doc['score']
        }
        for doc in relevant_docs
    ]
    
    return ChatResponse(
        question=req.question,
        answer=answer,
        sources=sources,
        context_used=context,
        session_id=req.session_id or str(uuid.uuid4())
    )
```

**üß™ Test Implementation:**
```python
@pytest.mark.integration
@pytest.mark.mock
def test_chat_with_document_context(mock_model_loader, mock_db_connection):
    # Arrange: Mock de b√∫squeda sem√°ntica
    mock_search_results = [
        {
            "document_id": "DOC-ARQ-001",
            "title": "Plan Maestro de Arquitectura",
            "snippet": "24 aulas, biblioteca, laboratorios...",
            "score": 0.87
        }
    ]
    
    # Mock del LLM (Groq)
    mock_groq_response = "Bas√°ndome en la documentaci√≥n, el Plan Maestro..."
    
    # Act: Ejecutar chat
    with patch('app.api.semantic_search', return_value=mock_search_results), \
         patch('app.api.Groq') as mock_groq_class, \
         patch.dict('os.environ', {'GROQ_API_KEY': 'test-key'}):
        
        # Configurar mock de Groq
        mock_groq_instance = MagicMock()
        mock_groq_class.return_value = mock_groq_instance
        
        mock_completion = MagicMock()
        mock_completion.choices = [MagicMock()]
        mock_completion.choices[0].message.content = mock_groq_response
        mock_groq_instance.chat.completions.create.return_value = mock_completion
        
        # Request
        request = ChatRequest(
            question="¬øQu√© incluye el plan maestro?",
            max_context_docs=5,
            session_id="test-session-001"
        )
        
        response = chat(request)
    
    # Assert: Verificar flujo completo
    
    # 1. Estructura de respuesta
    assert isinstance(response, ChatResponse)
    assert response.question == "¬øQu√© incluye el plan maestro?"
    assert len(response.answer) > 50
    
    # 2. Sources incluidas
    assert len(response.sources) > 0
    assert any("DOC-ARQ-001" in str(s) for s in response.sources)
    
    # 3. Context usado
    assert len(response.context_used) > 100
    assert "Plan Maestro" in response.context_used
    
    # 4. Session tracking
    assert response.session_id == "test-session-001"
```

**üîç Assertions T√©cnicas:**
1. **Retrieval**: `semantic_search()` llamado con query
2. **Filtering**: Solo docs con `score > 0.20`
3. **Augmentation**: Contexto formateado con separadores
4. **Generation**: Groq LLM con `temperature=0.3`
5. **Citation**: Lista de sources con IDs y scores

---

### Test 4.2: `test_save_chat_history` (Positivo)

**üéØ Objetivo:** Validar persistencia de conversaciones en PostgreSQL.

**üìç C√≥digo bajo test:**
```python
# Archivo: app/analytics.py
def save_chat_history(chat: ChatHistory):
    """
    Guarda conversaci√≥n en tabla chat_history.
    
    Schema:
    CREATE TABLE chat_history (
        id SERIAL PRIMARY KEY,
        user_id VARCHAR(255),
        question TEXT,
        answer TEXT,
        session_id VARCHAR(255),
        created_at TIMESTAMP DEFAULT NOW()
    )
    """
    conn = psycopg2.connect(os.environ.get('DATABASE_URL'))
    cursor = conn.cursor()
    
    # Crear tabla si no existe
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS chat_history (
            id SERIAL PRIMARY KEY,
            user_id VARCHAR(255),
            question TEXT,
            answer TEXT,
            session_id VARCHAR(255),
            created_at TIMESTAMP DEFAULT NOW()
        )
    """)
    
    # Insertar registro
    cursor.execute(
        "INSERT INTO chat_history (user_id, question, answer, session_id) VALUES (%s, %s, %s, %s)",
        (chat.user_id, chat.question, chat.answer, chat.session_id)
    )
    
    conn.commit()
    cursor.close()
    conn.close()
    
    return {"status": "success"}
```

**üß™ Test Implementation:**
```python
@pytest.mark.integration
@pytest.mark.db
@pytest.mark.mock
def test_save_chat_history(mock_db_connection):
    # Arrange
    mock_cursor = mock_db_connection.cursor.return_value
    
    chat_data = ChatHistory(
        user_id="user-123",
        question="¬øPlanos estructurales?",
        answer="Los planos incluyen...",
        session_id="session-abc"
    )
    
    # Act
    with patch('app.analytics.psycopg2.connect', return_value=mock_db_connection), \
         patch.dict('os.environ', {'DATABASE_URL': 'postgresql://test'}):
        result = save_chat_history(chat_data)
    
    # Assert: Verificar SQL ejecutado
    execute_calls = mock_cursor.execute.call_args_list
    
    # 1. Verificar CREATE TABLE
    create_sql = str(execute_calls[0][0][0])
    assert "CREATE TABLE IF NOT EXISTS chat_history" in create_sql
    assert "user_id" in create_sql
    assert "created_at TIMESTAMP DEFAULT NOW()" in create_sql
    
    # 2. Verificar INSERT
    insert_sql = str(execute_calls[1][0][0])
    insert_params = execute_calls[1][0][1]
    
    assert "INSERT INTO chat_history" in insert_sql
    assert insert_params[0] == "user-123"
    assert insert_params[1] == "¬øPlanos estructurales?"
    
    # 3. Verificar commit
    assert mock_db_connection.commit.called
```

**üîç Assertions T√©cnicas:**
1. **Schema creation**: `CREATE TABLE IF NOT EXISTS` idempotente
2. **SQL parametrizado**: `%s` previene SQL injection
3. **Transaction**: `conn.commit()` persiste cambios
4. **Timestamp autom√°tico**: `DEFAULT NOW()` en PostgreSQL

---

### Test 4.3: `test_get_chat_history` (Positivo)

**üéØ Objetivo:** Validar recuperaci√≥n de historial ordenado por fecha.

**üìç C√≥digo bajo test:**
```python
def get_chat_history(user_id: str, limit: int = 20):
    """
    Recupera historial de conversaciones de un usuario.
    Ordenado por fecha descendente (m√°s recientes primero).
    """
    conn = psycopg2.connect(os.environ.get('DATABASE_URL'))
    cursor = conn.cursor()
    
    cursor.execute(
        """
        SELECT question, answer, created_at
        FROM chat_history
        WHERE user_id = %s
        ORDER BY created_at DESC
        LIMIT %s
        """,
        (user_id, limit)
    )
    
    rows = cursor.fetchall()
    
    history = [
        {
            "question": row[0],
            "answer": row[1],
            "timestamp": row[2].isoformat()
        }
        for row in rows
    ]
    
    cursor.close()
    conn.close()
    
    return history
```

**üß™ Test Implementation:**
```python
def test_get_chat_history(mock_db_connection):
    # Arrange: Mock de resultados
    mock_cursor = mock_db_connection.cursor.return_value
    
    mock_history = [
        ("¬øConcreto?", "F'c=280 kg/cm¬≤...", "2024-11-26 14:30:00"),
        ("¬øAulas?", "24 aulas...", "2024-11-26 14:25:00"),
        ("¬øNorma?", "NSR-10...", "2024-11-26 14:20:00")
    ]
    
    mock_cursor.fetchall.return_value = mock_history
    
    # Act
    with patch('app.analytics.psycopg2.connect', return_value=mock_db_connection), \
         patch.dict('os.environ', {'DATABASE_URL': 'postgresql://test'}):
        history = get_chat_history(user_id="user-123", limit=10)
    
    # Assert: Verificar SQL
    execute_calls = mock_cursor.execute.call_args_list
    select_sql = str(execute_calls[0][0][0])
    select_params = execute_calls[0][0][1]
    
    # 1. Verificar ORDER BY DESC
    assert "ORDER BY created_at DESC" in select_sql
    
    # 2. Verificar LIMIT
    assert "LIMIT" in select_sql
    assert select_params[1] == 10
    
    # 3. Verificar ordenamiento (m√°s reciente primero)
    assert "14:30:00" in str(history[0])
    assert "14:20:00" in str(history[2])
```

**üîç Assertions T√©cnicas:**
1. **Filtro por usuario**: `WHERE user_id = %s`
2. **Ordenamiento DESC**: Conversaciones m√°s recientes primero
3. **L√≠mite**: Paginaci√≥n con `LIMIT`
4. **Formato ISO**: `timestamp.isoformat()` para JSON

---

### Test 4.4: `test_chat_with_empty_question` (Negativo)

**üéØ Objetivo:** Validar que Pydantic rechace preguntas vac√≠as.

**üìç C√≥digo bajo test:**
```python
# Archivo: app/api.py
class ChatRequest(BaseModel):
    question: str = Field(..., min_length=1)  # ‚Üê Validaci√≥n Pydantic
    max_context_docs: int = 5
    project_id: Optional[str] = None
    session_id: Optional[str] = None
```

**üß™ Test Implementation:**
```python
def test_chat_with_empty_question():
    # Act & Assert: Pydantic debe lanzar ValidationError
    with pytest.raises(pydantic_core.ValidationError) as exc_info:
        request = ChatRequest(
            question="",  # String vac√≠o
            max_context_docs=5
        )
    
    # Verificar tipo de error
    error = exc_info.value
    assert "string_too_short" in str(error)
    assert "question" in str(error)
```

**üîç Assertions T√©cnicas:**
1. **Validaci√≥n en punto de entrada**: Pydantic valida ANTES de l√≥gica
2. **`Field(min_length=1)`**: String no puede estar vac√≠o
3. **`ValidationError`**: Excepci√≥n espec√≠fica con detalle

---

## ‚öôÔ∏è ESCENARIO 5: UTILIDADES

### Test 5.1: `test_simple_chunk_with_overlap` (Positivo)

**üéØ Objetivo:** Validar chunking de texto con overlap para preservar contexto.

**üìç C√≥digo bajo test:**
```python
# Archivo: app/utils.py
def simple_chunk(text: str, size: int = 30, overlap: int = 10) -> List[str]:
    """
    Divide texto en chunks con overlap.
    
    Algoritmo:
    1. Dividir texto en palabras
    2. Crear ventanas deslizantes de 'size' palabras
    3. Retroceder 'overlap' palabras entre chunks
    
    Ejemplo: size=30, overlap=10
    Chunk 1: palabras [0:30]
    Chunk 2: palabras [20:50]  ‚Üê Retrocede 10 palabras
    Chunk 3: palabras [40:70]
    """
    words = text.split()
    chunks = []
    
    i = 0
    while i < len(words):
        chunk_words = words[i:i + size]
        chunks.append(' '.join(chunk_words))
        i += (size - overlap)  # Avanzar con overlap
    
    return chunks
```

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_simple_chunk_with_overlap():
    # Arrange: Texto largo
    text = "palabra " * 100  # 100 palabras
    
    # Act: Dividir con overlap
    chunks = simple_chunk(text, size=30, overlap=10)
    
    # Assert: Validar chunking
    
    # 1. N√∫mero de chunks
    # Con 100 palabras, size=30, overlap=10:
    # Chunk 1: [0:30], Chunk 2: [20:50], Chunk 3: [40:70], Chunk 4: [60:90], Chunk 5: [80:100]
    expected_chunks = math.ceil((100 - 30) / (30 - 10)) + 1
    assert len(chunks) == expected_chunks
    
    # 2. Tama√±o de chunks (excepto √∫ltimo)
    for i, chunk in enumerate(chunks[:-1]):
        assert len(chunk.split()) == 30
    
    # 3. Verificar overlap (palabras compartidas)
    # √öltimas 10 palabras de chunk[0] == Primeras 10 de chunk[1]
    words_chunk0 = chunks[0].split()
    words_chunk1 = chunks[1].split()
    
    assert words_chunk0[-10:] == words_chunk1[:10]  # Overlap verificado
```

**üîç Assertions T√©cnicas:**
1. **Ventana deslizante**: `i += (size - overlap)`
2. **Preservaci√≥n de contexto**: Palabras compartidas entre chunks
3. **C√°lculo de chunks**: `ceil((total - size) / (size - overlap)) + 1`

---

### Test 5.2: `test_get_db_connection_success` (Positivo)

**üéØ Objetivo:** Validar conexi√≥n exitosa a PostgreSQL.

**üìç C√≥digo bajo test:**
```python
def get_db_connection():
    """
    Crea conexi√≥n a PostgreSQL usando DATABASE_URL.
    """
    import psycopg2
    return psycopg2.connect(os.environ['DATABASE_URL'])
```

**üß™ Test Implementation:**
```python
@pytest.mark.integration
@pytest.mark.mock
def test_get_db_connection_success(mock_db_connection):
    # Act: Obtener conexi√≥n
    with patch('app.utils.psycopg2.connect', return_value=mock_db_connection), \
         patch.dict('os.environ', {'DATABASE_URL': 'postgresql://localhost/test'}):
        conn = get_db_connection()
    
    # Assert: Validar objeto conexi√≥n
    assert conn is not None
    assert hasattr(conn, 'cursor')  # M√©todo cursor existe
    assert hasattr(conn, 'commit')  # M√©todo commit existe
    assert hasattr(conn, 'rollback')  # M√©todo rollback existe
```

**üîç Assertions T√©cnicas:**
1. **Driver psycopg2**: Biblioteca PostgreSQL para Python
2. **Connection string**: `postgresql://user:pass@host:port/db`
3. **M√©todos requeridos**: `cursor()`, `commit()`, `rollback()`

---

### Test 5.3: `test_simple_chunk_invalid_parameters` (Negativo)

**üéØ Objetivo:** Validar que par√°metros inv√°lidos sean manejados.

**üß™ Test Implementation:**
```python
@pytest.mark.unit
def test_simple_chunk_invalid_parameters():
    text = "texto de prueba"
    
    # Caso 1: size = 0
    with pytest.raises((ValueError, Exception)):
        simple_chunk(text, size=0, overlap=5)
    
    # Caso 2: overlap > size
    with pytest.raises((ValueError, Exception)):
        simple_chunk(text, size=10, overlap=20)
    
    # Caso 3: size negativo
    with pytest.raises((ValueError, Exception)):
        simple_chunk(text, size=-10, overlap=5)
```

**üîç Assertions T√©cnicas:**
1. **Validaci√≥n de par√°metros**: `size > 0` y `overlap < size`
2. **`ValueError`**: Excepci√≥n est√°ndar para valores inv√°lidos
3. **Prevenci√≥n de loops infinitos**: `size <= 0` causar√≠a loop

---

### Test 5.4: `test_get_db_connection_invalid_credentials` (Negativo)

**üéØ Objetivo:** Validar que credenciales incorrectas lancen `OperationalError`.

**üß™ Test Implementation:**
```python
def test_get_db_connection_invalid_credentials(mock_db_connection):
    # Arrange: Mock que lanza error de autenticaci√≥n
    mock_db_connection.side_effect = psycopg2.OperationalError(
        "FATAL: password authentication failed"
    )
    
    # Act & Assert
    with patch('app.utils.psycopg2.connect', side_effect=mock_db_connection.side_effect), \
         patch.dict('os.environ', {'DATABASE_URL': 'postgresql://wrong:pass@localhost/db'}):
        
        with pytest.raises(psycopg2.OperationalError) as exc_info:
            get_db_connection()
        
        assert "authentication failed" in str(exc_info.value)
```

**üîç Assertions T√©cnicas:**
1. **`OperationalError`**: Error de conexi√≥n/autenticaci√≥n PostgreSQL
2. **Seguridad**: Credenciales incorrectas fallan expl√≠citamente
3. **Mensaje descriptivo**: Incluye raz√≥n del error

---

### Test 5.5: `test_get_db_connection_missing_env_vars` (Negativo)

**üéØ Objetivo:** Validar que falta de `DATABASE_URL` lance error.

**üß™ Test Implementation:**
```python
def test_get_db_connection_missing_env_vars():
    # Arrange: Remover DATABASE_URL del entorno
    with patch.dict('os.environ', {}, clear=True):
        
        # Act & Assert: Debe lanzar KeyError
        with pytest.raises(KeyError) as exc_info:
            get_db_connection()
        
        assert "DATABASE_URL" in str(exc_info.value)
```

**üîç Assertions T√©cnicas:**
1. **`KeyError`**: Variable de entorno no definida
2. **Fail-fast**: Mejor fallar en startup que en runtime
3. **Configuration validation**: Validar env vars al inicio

---

## üìä Resumen de Tecnolog√≠as y Patrones

### Tecnolog√≠as por Escenario

| Escenario | Tecnolog√≠as Clave |
|-----------|-------------------|
| **Ingesta** | `json`, `datetime`, `Dict`, `Generator` |
| **B√∫squeda** | `sentence-transformers`, `numpy`, `pgvector`, `psycopg2` |
| **Upload** | `hashlib.md5`, `open()`, `UTF-8 encoding` |
| **Chat** | `Groq API`, `FastAPI`, `Pydantic`, `UUID` |
| **Utilidades** | `psycopg2`, `string manipulation`, `list comprehensions` |

### Patrones de Dise√±o Aplicados

1. **Generator Pattern**: `iter_docs_from_file()` usa `yield` para eficiencia de memoria
2. **Repository Pattern**: Funciones de acceso a datos aisladas (`get_db_connection`)
3. **Strategy Pattern**: Detecci√≥n autom√°tica de formato JSON vs NDJSON
4. **Decorator Pattern**: `@pytest.mark.unit`, `@pytest.mark.integration`
5. **Factory Pattern**: `get_model()` para cargar SentenceTransformer
6. **Template Method**: Flujo RAG con pasos definidos

### M√©tricas de Cobertura

```
Total Tests: 25
‚îú‚îÄ‚îÄ Unit Tests: 15 (60%)
‚îú‚îÄ‚îÄ Integration Tests: 8 (32%)
‚îî‚îÄ‚îÄ Mocked DB Tests: 2 (8%)

L√≠neas de c√≥digo testeadas:
‚îú‚îÄ‚îÄ app/ingest.py: ~80% cobertura
‚îú‚îÄ‚îÄ app/search_core.py: ~75% cobertura
‚îú‚îÄ‚îÄ app/upload.py: ~70% cobertura
‚îú‚îÄ‚îÄ app/api.py (chat): ~65% cobertura
‚îî‚îÄ‚îÄ app/utils.py: ~85% cobertura
```

---

## üéì Conclusi√≥n T√©cnica

Este conjunto de tests implementa **testing piramidal**:
- Base: Unit tests (r√°pidos, aislados)
- Medio: Integration tests (mocked DB)
- Tope: End-to-end (flujo completo RAG)

**Ventajas de esta arquitectura:**
1. **Fast feedback**: Unit tests ejecutan en <1s
2. **Determinismo**: Mocks eliminan flakiness de BD/API externa
3. **Cobertura completa**: 25 tests cubren 5 escenarios cr√≠ticos
4. **Mantenibilidad**: Tests documentan comportamiento esperado
5. **CI/CD ready**: No requieren BD real para ejecutar

---

**Autor**: Luis Cornejo  
**Fecha**: Noviembre 27, 2025  
**Framework**: pytest 9.0.1  
**Python**: 3.11.0
