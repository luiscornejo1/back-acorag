# âš¡ GuÃ­a RÃ¡pida - EvaluaciÃ³n con RAGAS

## ğŸ¯ Â¿QuÃ© es RAGAS?

**RAGAS** (Retrieval Augmented Generation Assessment) es un framework especializado para evaluar sistemas RAG usando mÃ©tricas especÃ­ficas:

| MÃ©trica | Â¿QuÃ© evalÃºa? | Rango | Objetivo |
|---------|--------------|-------|----------|
| **Faithfulness** | Â¿Respuesta fiel al contexto? | 0-1 | > 0.7 |
| **Answer Relevancy** | Â¿Respuesta relevante? | 0-1 | > 0.7 |
| **Context Precision** | Â¿Retrieval preciso? | 0-1 | > 0.7 |
| **Context Recall** | Â¿Contexto completo? | 0-1 | > 0.7 |
| **Answer Similarity** | Similitud semÃ¡ntica (SemScore) | 0-1 | > 0.7 |
| **Answer Correctness** | Â¿Respuesta correcta? | 0-1 | > 0.7 |

---

## ğŸš€ Inicio RÃ¡pido (5 minutos)

### 1. Configurar API Key de OpenAI

RAGAS usa GPT-4 para evaluaciÃ³n (mÃ¡s preciso que mÃ©tricas tradicionales):

```bash
# Windows PowerShell
$env:OPENAI_API_KEY="sk-tu-api-key-aqui"

# Linux/Mac
export OPENAI_API_KEY="sk-tu-api-key-aqui"

# O crear archivo .env
echo "OPENAI_API_KEY=sk-tu-api-key-aqui" > .env
```

**ğŸ’¡ Obtener API Key:** https://platform.openai.com/api-keys

### 2. Instalar Dependencias

```bash
pip install ragas datasets transformers
```

### 3. Ejecutar EvaluaciÃ³n

```bash
cd backend-acorag

# EvaluaciÃ³n completa
python tests/test_ragas_evaluation.py

# Con pytest
pytest tests/test_ragas_evaluation.py -v
```

### 4. Ver Resultados

```bash
cat reports/ragas_evaluation.txt
```

---

## ğŸ“Š Resultados Esperados

```
ğŸ“Š RESUMEN DE EVALUACIÃ“N RAGAS
================================================================================
âœ… faithfulness              : 0.8500
âœ… answer_relevancy          : 0.8200
âœ… context_precision         : 0.7800
âœ… context_recall            : 0.7600
âœ… answer_similarity         : 0.8300
âœ… answer_correctness        : 0.8100
================================================================================
```

---

## ğŸ” ComparaciÃ³n: RAGAS vs MÃ©tricas BÃ¡sicas

### MÃ©tricas BÃ¡sicas (ya implementadas)

| MÃ©trica | QuÃ© mide | LimitaciÃ³n |
|---------|----------|------------|
| BERT Score | Similitud semÃ¡ntica | No evalÃºa fidelidad al contexto |
| ROUGE | Coincidencia lÃ©xica | No detecta alucinaciones |
| WER | Exactitud literal | Penaliza reformulaciones correctas |

**Resultado:** BERT 0.83, ROUGE-1 0.46 âœ… Sistema comprende bien

### MÃ©tricas RAGAS (nuevo)

| MÃ©trica | QuÃ© mide | Ventaja |
|---------|----------|---------|
| Faithfulness | Â¿Respuesta fiel a los documentos? | **Detecta alucinaciones** |
| Answer Relevancy | Â¿Respuesta relevante a la pregunta? | **EvalÃºa pertinencia** |
| Context Precision | Â¿Documentos recuperados son correctos? | **EvalÃºa retrieval** |
| Context Recall | Â¿Se recuperÃ³ toda la info necesaria? | **EvalÃºa completitud** |
| Answer Similarity | Similitud con respuesta ideal | Similar a BERT Score |

**Resultado:** EvalÃºa todo el pipeline RAG (retrieval + generation)

---

## ğŸ¯ Casos de Uso

### 1. Detectar Alucinaciones

```python
# Faithfulness < 0.5 â†’ Posibles alucinaciones
pytest tests/test_ragas_evaluation.py::test_ragas_faithfulness -v
```

**Si faithfulness es bajo:**
- El sistema inventa informaciÃ³n no presente en documentos
- AcciÃ³n: Ajustar prompt, limitar creatividad del LLM

### 2. Evaluar Retrieval

```python
# Context Precision < 0.5 â†’ Retrieval recupera documentos irrelevantes
pytest tests/test_ragas_evaluation.py::test_ragas_context_precision -v
```

**Si context_precision es bajo:**
- El sistema recupera documentos no relevantes
- AcciÃ³n: Mejorar embeddings, ajustar threshold de similitud

### 3. Comparar con Ground Truth

```python
# Answer Similarity â†’ SemScore de Hugging Face
pytest tests/test_ragas_evaluation.py::test_ragas_answer_similarity -v
```

**ComparaciÃ³n:**
- BERT Score (bÃ¡sico): 0.8335
- Answer Similarity (RAGAS): ~0.83
- Ambos deben ser consistentes

---

## ğŸ’¡ InterpretaciÃ³n de Resultados

### Faithfulness (Fidelidad)

```
0.9-1.0  ğŸŸ¢ EXCELENTE  - Sin alucinaciones
0.7-0.9  ğŸŸ¢ BUENA      - Ocasionalmente aÃ±ade info
0.5-0.7  ğŸŸ¡ ACEPTABLE  - Algunas inconsistencias
< 0.5    ğŸ”´ REVISAR    - Alucinaciones frecuentes
```

**Ejemplo:**
```
Contexto: "El sistema usa PostgreSQL con pgvector"
Respuesta: "El sistema usa PostgreSQL con pgvector"  â†’ Faithfulness: 1.0 âœ…
Respuesta: "El sistema usa MongoDB"                  â†’ Faithfulness: 0.0 âŒ
```

### Answer Relevancy (Relevancia)

```
0.9-1.0  ğŸŸ¢ EXCELENTE  - Directamente responde la pregunta
0.7-0.9  ğŸŸ¢ BUENA      - Responde con info adicional
0.5-0.7  ğŸŸ¡ ACEPTABLE  - Parcialmente off-topic
< 0.5    ğŸ”´ REVISAR    - Respuesta no relacionada
```

**Ejemplo:**
```
Pregunta: "Â¿QuÃ© base de datos usa?"
Respuesta: "PostgreSQL con pgvector"                     â†’ Relevancy: 1.0 âœ…
Respuesta: "El sistema tiene mÃºltiples componentes..."   â†’ Relevancy: 0.6 âš ï¸
```

### Context Precision (PrecisiÃ³n del Retrieval)

```
0.9-1.0  ğŸŸ¢ EXCELENTE  - Solo docs relevantes
0.7-0.9  ğŸŸ¢ BUENA      - MayorÃ­a relevantes
0.5-0.7  ğŸŸ¡ ACEPTABLE  - Algo de ruido
< 0.5    ğŸ”´ REVISAR    - Demasiados docs irrelevantes
```

**QuÃ© mide:**
- Si los documentos recuperados son realmente relevantes
- Bajo = retrieval recupera mucho ruido

### Context Recall (Completitud)

```
0.9-1.0  ğŸŸ¢ EXCELENTE  - RecuperÃ³ toda la info necesaria
0.7-0.9  ğŸŸ¢ BUENA      - RecuperÃ³ la mayorÃ­a
0.5-0.7  ğŸŸ¡ ACEPTABLE  - Falta alguna info
< 0.5    ğŸ”´ REVISAR    - Falta mucha informaciÃ³n
```

**QuÃ© mide:**
- Si se recuperaron TODOS los documentos necesarios
- Bajo = retrieval no es exhaustivo

---

## ğŸ”§ Comandos Ãštiles

### Ejecutar Tests Individuales

```bash
# Solo Faithfulness
pytest tests/test_ragas_evaluation.py::test_ragas_faithfulness -v

# Solo Answer Relevancy
pytest tests/test_ragas_evaluation.py::test_ragas_answer_relevancy -v

# Solo Context Precision
pytest tests/test_ragas_evaluation.py::test_ragas_context_precision -v

# Solo Answer Similarity (SemScore)
pytest tests/test_ragas_evaluation.py::test_ragas_answer_similarity -v
```

### Ver Reportes

```bash
# Reporte completo
cat reports/ragas_evaluation.txt

# CSV para anÃ¡lisis
cat reports/ragas_results.csv
```

---

## ğŸ“Š Estructura del Dataset

RAGAS necesita 4 campos para cada caso:

```python
{
    "question": "Â¿QuÃ© es el sistema?",           # La pregunta del usuario
    "answer": "Es un sistema RAG...",            # Respuesta del sistema
    "contexts": ["Contexto doc 1", "Doc 2"],     # Documentos recuperados
    "ground_truth": "Respuesta ideal..."         # Respuesta de referencia
}
```

**Diferencia con mÃ©tricas bÃ¡sicas:**
- MÃ©tricas bÃ¡sicas: Solo question + answer + ground_truth
- RAGAS: **Incluye contexts** â†’ Puede evaluar el retrieval

---

## âš ï¸ Troubleshooting

### Error: "OPENAI_API_KEY not configured"

```bash
# Verificar que estÃ© configurada
echo $env:OPENAI_API_KEY  # Windows
echo $OPENAI_API_KEY      # Linux/Mac

# Si no estÃ¡, configurarla
$env:OPENAI_API_KEY="sk-..."  # Windows
export OPENAI_API_KEY="sk-..."  # Linux/Mac
```

### Error: "Rate limit exceeded"

RAGAS hace mÃºltiples llamadas a GPT-4. Si tienes lÃ­mite:

```python
# Usar modelo mÃ¡s barato
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# O evaluar menos casos
RAGAS_DATASET = {...}  # Reducir a 3-5 casos
```

### EvaluaciÃ³n muy lenta

```python
# Evaluar solo algunas mÃ©tricas
resultado = evaluar_con_ragas(dataset, metricas=[
    faithfulness,        # La mÃ¡s importante
    answer_relevancy,    # TambiÃ©n crÃ­tica
])
```

---

## ğŸ“ˆ Roadmap

### Ya implementado âœ…
- [x] MÃ©tricas bÃ¡sicas (BERT, ROUGE, WER)
- [x] RAGAS con 6 mÃ©tricas
- [x] Dataset de 8 casos

### PrÃ³ximos pasos ğŸ“‹
- [ ] Aumentar dataset a 20 casos
- [ ] Integrar evaluaciÃ³n continua (CI/CD)
- [ ] Dashboard visual de mÃ©tricas
- [ ] ComparaciÃ³n temporal (tracking)

---

## ğŸ”— Referencias

- [RAGAS Docs](https://docs.ragas.io/)
- [Paper RAGAS](https://arxiv.org/abs/2309.15217)
- [Hugging Face SemScore](https://huggingface.co/spaces/evaluate-metric/semscore)
- [ComparaciÃ³n mÃ©tricas RAG](https://docs.ragas.io/en/latest/concepts/metrics/index.html)

---

## ğŸ“ ComparaciÃ³n Final

| Aspecto | MÃ©tricas BÃ¡sicas | RAGAS |
|---------|------------------|-------|
| **EvalÃºa generaciÃ³n** | âœ… SÃ­ | âœ… SÃ­ |
| **EvalÃºa retrieval** | âŒ No | âœ… SÃ­ |
| **Detecta alucinaciones** | âŒ No | âœ… SÃ­ |
| **EvalÃºa relevancia** | âš ï¸ Parcial | âœ… SÃ­ |
| **Costo** | ğŸŸ¢ Gratis | ğŸŸ¡ API OpenAI |
| **Velocidad** | ğŸŸ¢ RÃ¡pido | ğŸŸ¡ Lento |
| **PrecisiÃ³n** | ğŸŸ¡ Buena | ğŸŸ¢ Excelente |

**RecomendaciÃ³n:**
- **MÃ©tricas bÃ¡sicas:** Para desarrollo rÃ¡pido, CI/CD
- **RAGAS:** Para evaluaciÃ³n exhaustiva, pre-deployment

---

**Generado:** Diciembre 2024  
**Tiempo de lectura:** 5 minutos  
**Nivel:** Intermedio
